{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8b8efd",
   "metadata": {},
   "source": [
    "# METR-LA Traffic Dataset Analysis\n",
    "\n",
    "**Task 6.2: Data Analysis Notebook**  \n",
    "Traffic Prediction System - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of the METR-LA traffic dataset including:\n",
    "- Dataset overview and structure\n",
    "- Sensor location analysis (207 sensors)\n",
    "- Traffic pattern analysis\n",
    "- Missing data patterns\n",
    "- Data quality assessment\n",
    "- Statistical insights and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17226899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253ac13",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Overview\n",
    "\n",
    "Load the METR-LA dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path(\"../data/raw\")\n",
    "\n",
    "# Load sensor metadata\n",
    "metadata_file = data_dir / \"metr_la_sensor_metadata.csv\"\n",
    "sensors_df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Load traffic data (use sample for this analysis)\n",
    "sample_file = data_dir / \"metr_la_sample_data.csv\"\n",
    "if sample_file.exists():\n",
    "    traffic_df = pd.read_csv(sample_file)\n",
    "    print(\"üìä Using sample dataset for analysis\")\n",
    "else:\n",
    "    # Try to load full dataset if available\n",
    "    full_file = data_dir / \"metr_la_traffic_data.csv\"\n",
    "    if full_file.exists():\n",
    "        print(\"üìä Loading full dataset (this may take a moment...)\")\n",
    "        traffic_df = pd.read_csv(full_file)\n",
    "    else:\n",
    "        print(\"‚ùå No traffic data found. Please run the dataset generation script first.\")\n",
    "        raise FileNotFoundError(\"Traffic dataset not found\")\n",
    "\n",
    "print(f\"\\nüìà Dataset Overview:\")\n",
    "print(f\"   Sensors: {len(sensors_df):,}\")\n",
    "print(f\"   Traffic Records: {len(traffic_df):,}\")\n",
    "print(f\"   Memory Usage: {traffic_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63948099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "print(\"üìã Sensor Metadata Structure:\")\n",
    "print(sensors_df.info())\n",
    "print(\"\\nüìä First 5 sensors:\")\n",
    "print(sensors_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Traffic Data Structure:\")\n",
    "print(traffic_df.info())\n",
    "print(\"\\nüìä First 5 traffic records:\")\n",
    "print(traffic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b5d03",
   "metadata": {},
   "source": [
    "## 2. Sensor Location Analysis\n",
    "\n",
    "Analyze the 207 sensor locations across Los Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor distribution by road type\n",
    "road_type_counts = sensors_df['road_type'].value_counts()\n",
    "print(\"üõ£Ô∏è Sensor Distribution by Road Type:\")\n",
    "for road_type, count in road_type_counts.items():\n",
    "    percentage = (count / len(sensors_df)) * 100\n",
    "    print(f\"   {road_type.title()}: {count} sensors ({percentage:.1f}%)\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart of road types\n",
    "ax1.pie(road_type_counts.values, labels=road_type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Sensor Distribution by Road Type')\n",
    "\n",
    "# Lane distribution\n",
    "lane_dist = sensors_df.groupby('road_type')['lanes'].mean()\n",
    "ax2.bar(lane_dist.index, lane_dist.values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax2.set_title('Average Lanes by Road Type')\n",
    "ax2.set_ylabel('Average Number of Lanes')\n",
    "ax2.set_xlabel('Road Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9dfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution analysis\n",
    "print(\"üó∫Ô∏è Geographic Distribution:\")\n",
    "print(f\"   Latitude range: {sensors_df['latitude'].min():.4f} to {sensors_df['latitude'].max():.4f}\")\n",
    "print(f\"   Longitude range: {sensors_df['longitude'].min():.4f} to {sensors_df['longitude'].max():.4f}\")\n",
    "print(f\"   Geographic span: ~{((sensors_df['latitude'].max() - sensors_df['latitude'].min()) * 69):.1f} miles N-S\")\n",
    "print(f\"                   ~{((sensors_df['longitude'].max() - sensors_df['longitude'].min()) * 54.6):.1f} miles E-W\")\n",
    "\n",
    "# Plot sensor locations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create scatter plot colored by road type\n",
    "colors = {'highway': 'red', 'arterial': 'blue', 'local': 'green'}\n",
    "for road_type in sensors_df['road_type'].unique():\n",
    "    subset = sensors_df[sensors_df['road_type'] == road_type]\n",
    "    plt.scatter(subset['longitude'], subset['latitude'], \n",
    "               c=colors[road_type], label=road_type.title(), \n",
    "               alpha=0.7, s=60)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('METR-LA Traffic Sensor Locations in Los Angeles')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìç Sensor density: {len(sensors_df) / ((sensors_df['latitude'].max() - sensors_df['latitude'].min()) * (sensors_df['longitude'].max() - sensors_df['longitude'].min())):.1f} sensors per square degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71dc7b",
   "metadata": {},
   "source": [
    "## 3. Traffic Pattern Analysis\n",
    "\n",
    "Analyze traffic speed and volume patterns across different road types and time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "traffic_df['timestamp'] = pd.to_datetime(traffic_df['timestamp'])\n",
    "traffic_df['hour'] = traffic_df['timestamp'].dt.hour\n",
    "traffic_df['day_of_week'] = traffic_df['timestamp'].dt.day_name()\n",
    "traffic_df['weekday'] = traffic_df['timestamp'].dt.weekday < 5\n",
    "\n",
    "# Overall statistics\n",
    "print(\"üöó Traffic Statistics Overview:\")\n",
    "print(f\"   Date range: {traffic_df['timestamp'].min()} to {traffic_df['timestamp'].max()}\")\n",
    "print(f\"   Unique sensors in data: {traffic_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Total measurements: {len(traffic_df):,}\")\n",
    "\n",
    "print(\"\\nüèéÔ∏è Speed Statistics:\")\n",
    "speed_stats = traffic_df['speed_mph'].describe()\n",
    "for stat, value in speed_stats.items():\n",
    "    print(f\"   {stat.title()}: {value:.2f} mph\")\n",
    "\n",
    "print(\"\\nüöõ Volume Statistics:\")\n",
    "volume_stats = traffic_df['volume_vehicles_per_hour'].describe()\n",
    "for stat, value in volume_stats.items():\n",
    "    print(f\"   {stat.title()}: {value:.0f} vehicles/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly traffic patterns\n",
    "hourly_stats = traffic_df.groupby('hour').agg({\n",
    "    'speed_mph': ['mean', 'std'],\n",
    "    'volume_vehicles_per_hour': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "hourly_stats.columns = ['_'.join(col) for col in hourly_stats.columns]\n",
    "\n",
    "# Plot hourly patterns\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Speed patterns\n",
    "ax1.plot(hourly_stats.index, hourly_stats['speed_mph_mean'], 'b-', linewidth=2, label='Average Speed')\n",
    "ax1.fill_between(hourly_stats.index, \n",
    "                 hourly_stats['speed_mph_mean'] - hourly_stats['speed_mph_std'],\n",
    "                 hourly_stats['speed_mph_mean'] + hourly_stats['speed_mph_std'],\n",
    "                 alpha=0.3, label='¬±1 Std Dev')\n",
    "ax1.set_title('Average Traffic Speed by Hour of Day')\n",
    "ax1.set_xlabel('Hour of Day')\n",
    "ax1.set_ylabel('Speed (mph)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Volume patterns\n",
    "ax2.plot(hourly_stats.index, hourly_stats['volume_vehicles_per_hour_mean'], 'r-', linewidth=2, label='Average Volume')\n",
    "ax2.fill_between(hourly_stats.index,\n",
    "                 hourly_stats['volume_vehicles_per_hour_mean'] - hourly_stats['volume_vehicles_per_hour_std'],\n",
    "                 hourly_stats['volume_vehicles_per_hour_mean'] + hourly_stats['volume_vehicles_per_hour_std'],\n",
    "                 alpha=0.3, label='¬±1 Std Dev')\n",
    "ax2.set_title('Average Traffic Volume by Hour of Day')\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Volume (vehicles/hour)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify rush hours\n",
    "min_speed_hours = hourly_stats['speed_mph_mean'].nsmallest(3)\n",
    "print(\"‚è∞ Rush Hour Analysis (lowest average speeds):\")\n",
    "for hour, speed in min_speed_hours.items():\n",
    "    print(f\"   {hour}:00 - {speed:.1f} mph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic patterns by road type\n",
    "road_type_stats = traffic_df.groupby('road_type').agg({\n",
    "    'speed_mph': ['mean', 'std', 'min', 'max'],\n",
    "    'volume_vehicles_per_hour': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "print(\"üõ£Ô∏è Traffic Patterns by Road Type:\")\n",
    "print(road_type_stats)\n",
    "\n",
    "# Visualize road type comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Speed comparison\n",
    "speed_data = [traffic_df[traffic_df['road_type'] == rt]['speed_mph'].dropna() for rt in ['highway', 'arterial', 'local']]\n",
    "ax1.boxplot(speed_data, labels=['Highway', 'Arterial', 'Local'])\n",
    "ax1.set_title('Speed Distribution by Road Type')\n",
    "ax1.set_ylabel('Speed (mph)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Volume comparison\n",
    "volume_data = [traffic_df[traffic_df['road_type'] == rt]['volume_vehicles_per_hour'].dropna() for rt in ['highway', 'arterial', 'local']]\n",
    "ax2.boxplot(volume_data, labels=['Highway', 'Arterial', 'Local'])\n",
    "ax2.set_title('Volume Distribution by Road Type')\n",
    "ax2.set_ylabel('Volume (vehicles/hour)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b451709",
   "metadata": {},
   "source": [
    "## 4. Missing Data Analysis\n",
    "\n",
    "Analyze missing data patterns to understand sensor reliability and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall missing data statistics\n",
    "total_records = len(traffic_df)\n",
    "missing_speed = traffic_df['speed_mph'].isna().sum()\n",
    "missing_volume = traffic_df['volume_vehicles_per_hour'].isna().sum()\n",
    "\n",
    "print(\"‚ùì Missing Data Overview:\")\n",
    "print(f\"   Total records: {total_records:,}\")\n",
    "print(f\"   Missing speed readings: {missing_speed:,} ({missing_speed/total_records*100:.2f}%)\")\n",
    "print(f\"   Missing volume readings: {missing_volume:,} ({missing_volume/total_records*100:.2f}%)\")\n",
    "print(f\"   Complete records: {total_records - missing_speed:,} ({(total_records - missing_speed)/total_records*100:.2f}%)\")\n",
    "\n",
    "# Missing data by sensor\n",
    "sensor_missing = traffic_df.groupby('sensor_id').agg({\n",
    "    'speed_mph': lambda x: x.isna().sum(),\n",
    "    'timestamp': 'count'\n",
    "}).rename(columns={'speed_mph': 'missing_count', 'timestamp': 'total_count'})\n",
    "\n",
    "sensor_missing['missing_rate'] = (sensor_missing['missing_count'] / sensor_missing['total_count'] * 100).round(2)\n",
    "sensor_missing = sensor_missing.sort_values('missing_rate', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Sensor Reliability (Top 10 sensors with most missing data):\")\n",
    "print(sensor_missing.head(10))\n",
    "\n",
    "print(f\"\\nüèÜ Best performing sensors (lowest missing data rates):\")\n",
    "print(sensor_missing.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Missing data rate distribution\n",
    "ax1.hist(sensor_missing['missing_rate'], bins=20, edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Distribution of Missing Data Rates Across Sensors')\n",
    "ax1.set_xlabel('Missing Data Rate (%)')\n",
    "ax1.set_ylabel('Number of Sensors')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Missing data by hour of day\n",
    "hourly_missing = traffic_df.groupby('hour')['speed_mph'].apply(lambda x: x.isna().sum())\n",
    "total_by_hour = traffic_df.groupby('hour').size()\n",
    "hourly_missing_rate = (hourly_missing / total_by_hour * 100).round(2)\n",
    "\n",
    "ax2.bar(hourly_missing_rate.index, hourly_missing_rate.values, color='coral', alpha=0.7)\n",
    "ax2.set_title('Missing Data Rate by Hour of Day')\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Missing Data Rate (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚è∞ Peak missing data hours:\")\n",
    "peak_missing_hours = hourly_missing_rate.nlargest(3)\n",
    "for hour, rate in peak_missing_hours.items():\n",
    "    print(f\"   {hour}:00 - {rate:.2f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee5188",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment\n",
    "\n",
    "Assess overall data quality and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"üîç Data Quality Assessment:\")\n",
    "\n",
    "# 1. Speed value ranges\n",
    "valid_speeds = traffic_df['speed_mph'].dropna()\n",
    "invalid_speeds = valid_speeds[(valid_speeds < 0) | (valid_speeds > 100)]\n",
    "print(f\"\\nüèéÔ∏è Speed Data Quality:\")\n",
    "print(f\"   Valid speed readings: {len(valid_speeds):,}\")\n",
    "print(f\"   Invalid speeds (<0 or >100 mph): {len(invalid_speeds)} ({len(invalid_speeds)/len(valid_speeds)*100:.3f}%)\")\n",
    "print(f\"   Speed range: {valid_speeds.min():.1f} to {valid_speeds.max():.1f} mph\")\n",
    "\n",
    "# 2. Volume value ranges\n",
    "valid_volumes = traffic_df['volume_vehicles_per_hour'].dropna()\n",
    "invalid_volumes = valid_volumes[valid_volumes < 0]\n",
    "print(f\"\\nüöõ Volume Data Quality:\")\n",
    "print(f\"   Valid volume readings: {len(valid_volumes):,}\")\n",
    "print(f\"   Invalid volumes (<0): {len(invalid_volumes)} ({len(invalid_volumes)/len(valid_volumes)*100:.3f}%)\")\n",
    "print(f\"   Volume range: {valid_volumes.min():.0f} to {valid_volumes.max():.0f} vehicles/hour\")\n",
    "\n",
    "# 3. Timestamp continuity\n",
    "print(f\"\\nüìÖ Temporal Data Quality:\")\n",
    "time_diffs = traffic_df.groupby('sensor_id')['timestamp'].apply(lambda x: x.sort_values().diff().dt.total_seconds().median())\n",
    "expected_interval = 300  # 5 minutes in seconds\n",
    "print(f\"   Expected measurement interval: {expected_interval} seconds (5 minutes)\")\n",
    "print(f\"   Median actual interval: {time_diffs.median():.0f} seconds\")\n",
    "print(f\"   Sensors with correct timing: {(time_diffs == expected_interval).sum()}/{len(time_diffs)}\")\n",
    "\n",
    "# 4. Duplicate records\n",
    "duplicates = traffic_df.duplicated(['timestamp', 'sensor_id']).sum()\n",
    "print(f\"\\nüîÑ Data Consistency:\")\n",
    "print(f\"   Duplicate records: {duplicates} ({duplicates/len(traffic_df)*100:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"üìä Variable Correlation Analysis:\")\n",
    "\n",
    "# Calculate correlations\n",
    "numeric_cols = ['speed_mph', 'volume_vehicles_per_hour', 'hour', 'latitude', 'longitude']\n",
    "correlation_matrix = traffic_df[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Correlation Matrix of Traffic Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key correlations\n",
    "speed_volume_corr = traffic_df['speed_mph'].corr(traffic_df['volume_vehicles_per_hour'])\n",
    "print(f\"\\nüîó Key Relationships:\")\n",
    "print(f\"   Speed vs Volume correlation: {speed_volume_corr:.3f}\")\n",
    "if speed_volume_corr < -0.3:\n",
    "    print(\"   ‚û° Strong negative correlation: Higher volume leads to lower speeds\")\n",
    "elif speed_volume_corr < -0.1:\n",
    "    print(\"   ‚û° Moderate negative correlation: Some relationship between volume and speed\")\n",
    "else:\n",
    "    print(\"   ‚û° Weak correlation: Volume and speed relationship is complex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbd164",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Insights\n",
    "\n",
    "Summarize the key findings from the METR-LA dataset analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ad6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis summary\n",
    "analysis_summary = {\n",
    "    \"analysis_metadata\": {\n",
    "        \"analysis_date\": datetime.now().isoformat(),\n",
    "        \"dataset_type\": \"METR-LA Traffic Dataset\",\n",
    "        \"analyst\": \"Traffic Prediction System\",\n",
    "        \"analysis_version\": \"1.0\"\n",
    "    },\n",
    "    \"dataset_overview\": {\n",
    "        \"total_sensors\": len(sensors_df),\n",
    "        \"sensors_in_traffic_data\": traffic_df['sensor_id'].nunique(),\n",
    "        \"total_traffic_records\": len(traffic_df),\n",
    "        \"date_range\": {\n",
    "            \"start\": traffic_df['timestamp'].min().isoformat(),\n",
    "            \"end\": traffic_df['timestamp'].max().isoformat(),\n",
    "            \"duration_days\": (traffic_df['timestamp'].max() - traffic_df['timestamp'].min()).days\n",
    "        },\n",
    "        \"measurement_frequency\": \"5 minutes\"\n",
    "    },\n",
    "    \"sensor_distribution\": {\n",
    "        \"by_road_type\": road_type_counts.to_dict(),\n",
    "        \"geographic_coverage\": {\n",
    "            \"latitude_range\": [float(sensors_df['latitude'].min()), float(sensors_df['latitude'].max())],\n",
    "            \"longitude_range\": [float(sensors_df['longitude'].min()), float(sensors_df['longitude'].max())],\n",
    "            \"coverage_area_sq_miles\": round(((sensors_df['latitude'].max() - sensors_df['latitude'].min()) * 69) * \n",
    "                                           ((sensors_df['longitude'].max() - sensors_df['longitude'].min()) * 54.6), 1)\n",
    "        }\n",
    "    },\n",
    "    \"traffic_patterns\": {\n",
    "        \"speed_statistics\": {\n",
    "            \"overall_mean_mph\": round(traffic_df['speed_mph'].mean(), 2),\n",
    "            \"overall_std_mph\": round(traffic_df['speed_mph'].std(), 2),\n",
    "            \"min_mph\": round(traffic_df['speed_mph'].min(), 2),\n",
    "            \"max_mph\": round(traffic_df['speed_mph'].max(), 2)\n",
    "        },\n",
    "        \"volume_statistics\": {\n",
    "            \"overall_mean_vph\": round(traffic_df['volume_vehicles_per_hour'].mean(), 2),\n",
    "            \"overall_std_vph\": round(traffic_df['volume_vehicles_per_hour'].std(), 2),\n",
    "            \"min_vph\": round(traffic_df['volume_vehicles_per_hour'].min(), 2),\n",
    "            \"max_vph\": round(traffic_df['volume_vehicles_per_hour'].max(), 2)\n",
    "        },\n",
    "        \"rush_hour_analysis\": {\n",
    "            \"lowest_speed_hours\": [int(hour) for hour in min_speed_hours.index[:3]],\n",
    "            \"peak_missing_data_hours\": [int(hour) for hour in peak_missing_hours.index[:3]]\n",
    "        },\n",
    "        \"road_type_performance\": road_type_stats.to_dict()\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_data\": {\n",
    "            \"overall_missing_rate_percent\": round(missing_speed / total_records * 100, 2),\n",
    "            \"sensors_with_high_missing_rate\": int((sensor_missing['missing_rate'] > 10).sum()),\n",
    "            \"sensors_with_low_missing_rate\": int((sensor_missing['missing_rate'] < 5).sum()),\n",
    "            \"median_sensor_missing_rate\": round(sensor_missing['missing_rate'].median(), 2)\n",
    "        },\n",
    "        \"data_validity\": {\n",
    "            \"invalid_speed_readings\": len(invalid_speeds),\n",
    "            \"invalid_volume_readings\": len(invalid_volumes),\n",
    "            \"duplicate_records\": duplicates\n",
    "        },\n",
    "        \"temporal_consistency\": {\n",
    "            \"median_measurement_interval_seconds\": float(time_diffs.median()),\n",
    "            \"sensors_with_correct_timing\": int((time_diffs == expected_interval).sum())\n",
    "        }\n",
    "    },\n",
    "    \"key_insights\": [\n",
    "        f\"Dataset covers {len(sensors_df)} sensors across Los Angeles with {traffic_df['sensor_id'].nunique()} sensors having traffic data\",\n",
    "        f\"Missing data rate is {missing_speed / total_records * 100:.1f}%, indicating {'good' if missing_speed / total_records < 0.1 else 'moderate'} data quality\",\n",
    "        f\"Clear rush hour patterns observed with lowest speeds at {min_speed_hours.index[0]}:00 ({min_speed_hours.iloc[0]:.1f} mph average)\",\n",
    "        f\"Highway sensors show highest speeds (avg {road_type_stats.loc['highway', ('speed_mph', 'mean')]:.1f} mph) vs local roads ({road_type_stats.loc['local', ('speed_mph', 'mean')]:.1f} mph)\",\n",
    "        f\"Speed-volume correlation of {speed_volume_corr:.3f} indicates {'strong' if abs(speed_volume_corr) > 0.5 else 'moderate' if abs(speed_volume_corr) > 0.3 else 'weak'} relationship\"\n",
    "    ],\n",
    "    \"recommendations\": [\n",
    "        \"Implement forward-fill imputation for missing speed/volume data\",\n",
    "        \"Focus preprocessing on rush hour periods (7-9 AM, 5-7 PM) for traffic prediction models\",\n",
    "        \"Consider road type as a key feature for traffic prediction algorithms\",\n",
    "        \"Monitor sensor reliability and flag sensors with >10% missing data for maintenance\",\n",
    "        \"Use 5-minute aggregation windows to match native measurement frequency\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save analysis summary\n",
    "summary_file = Path(\"../data/processed/metr_la_analysis_summary.json\")\n",
    "summary_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ METR-LA Dataset Analysis Complete!\")\n",
    "print(f\"üìã Analysis summary saved to: {summary_file}\")\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "for i, insight in enumerate(analysis_summary['key_insights'], 1):\n",
    "    print(f\"   {i}. {insight}\")\n",
    "\n",
    "print(\"\\nüí° Recommendations for Data Preprocessing:\")\n",
    "for i, rec in enumerate(analysis_summary['recommendations'], 1):\n",
    "    print(f\"   {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535d46c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this analysis, the next steps in the data pipeline are:\n",
    "\n",
    "1. **Data Preprocessing (Task 6.3)**: Clean and preprocess the dataset based on identified patterns\n",
    "2. **HDFS Upload (Task 6.4)**: Upload the cleaned dataset to HDFS for big data processing\n",
    "3. **Feature Engineering**: Create additional features based on rush hour patterns and road types\n",
    "4. **Model Training**: Use the insights to inform traffic prediction model development\n",
    "\n",
    "The analysis shows that the METR-LA dataset has good coverage across Los Angeles with reasonable data quality, making it suitable for traffic prediction model training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
