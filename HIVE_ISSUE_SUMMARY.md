# Hive Docker Issues - Summary and Workarounds

## ‚ùå Problem: Hive Not Working in Current Docker Setup

### Root Cause Analysis

The `bde2020/hive:2.3.2-postgresql-metastore` Docker image has a critical bug where:

1. **VERSION Table Issue**: Even after manually inserting version `2.3.0` into PostgreSQL's VERSION table, Hive still reports "Version information not found in metastore"

2. **The Bug**: The Hive ObjectStore.checkSchema() method queries the VERSION table but doesn't properly use the PostgreSQL connection parameters passed via environment variables

3. **Evidence**:
   - PostgreSQL contains all 57 Hive metastore tables ‚úÖ
   - VERSION table exists and contains version 2.3.0 ‚úÖ  
   - Hive metastore still crashes on startup with "Version information not found" ‚ùå

### Failed Attempts
- ‚úÖ Fixed docker-compose.yml port references
- ‚úÖ Removed SERVICE_PRECONDITION DNS checks
- ‚úÖ Manually initialized PostgreSQL schema
- ‚úÖ Manually inserted VERSION row
- ‚úÖ Added explicit PostgreSQL connection environment variables
- ‚ùå **Still failing** - The bde2020/hive image is fundamentally broken

## ‚úÖ Recommended Solutions

### Option 1: Use Alternative Hive Docker Image (RECOMMENDED)

**Apache Hive Official Image** or **Teradatalabs Hive**:

```yaml
  hive-metastore:
    image: apache/hive:3.1.3
    # OR
    # image: teradatalabs/hive:latest
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver 
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-postgresql:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive
    ports:
      - 9083:9083
    depends_on:
      - hive-metastore-postgresql

  hive-server:
    image: apache/hive:3.1.3
    container_name: hive-server
    environment:
      - SERVICE_NAME=hiveserver2
      - SERVICE_OPTS=-Dhive.metastore.uris=thrift://hive-metastore:9083
    ports:
      - 10000:10000
      - 10002:10002
    depends_on:
      - hive-metastore
```

### Option 2: Skip Hive Entirely (PRAGMATIC APPROACH)

For your ML pipeline, **you don't actually need Hive!** You can:

1. **Use Spark SQL directly on Parquet files** stored in HDFS
2. **Query with PySpark** without Hive metastore
3. **Save time** and avoid this broken Docker image

**Modified ML Pipeline Without Hive:**

```python
from pyspark.sql import SparkSession

# Create Spark session WITHOUT Hive
spark = SparkSession.builder \
    .appName("Traffic ML Training") \
    .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/warehouse") \
    .getOrCreate()

# Read Parquet files directly from HDFS
df = spark.read.parquet("hdfs://namenode:9000/traffic/events/2025/10/06")

# Create temporary view for SQL queries
df.createOrReplaceTempView("traffic_events")

# Query with SQL (no Hive needed!)
result = spark.sql("""
    SELECT segment_id, AVG(speed) as avg_speed
    FROM traffic_events
    GROUP BY segment_id
""")

# Train ML model
from pyspark.ml.regression import RandomForestRegressor
rf = RandomForestRegressor(featuresCol="features", labelCol="speed")
model = rf.fit(result)
```

### Option 3: Use Hive Standalone (Outside Docker)

Install Hive directly on Windows with PostgreSQL:

```powershell
# Download Hive 3.1.3
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# Extract and configure
# Edit conf/hive-site.xml with PostgreSQL connection

# Start metastore
bin/hive --service metastore

# Start HiveServer2
bin/hive --service hiveserver2
```

## üéØ Recommended Path Forward

### For Your Traffic Prediction Project:

**I recommend Option 2 - Skip Hive and use Spark SQL directly.**

**Why?**
1. ‚úÖ **Faster to implement** - No need to fix broken Docker images
2. ‚úÖ **Simpler architecture** - One less service to maintain
3. ‚úÖ **Same capabilities** - Spark SQL can do everything Hive can
4. ‚úÖ **Better performance** - Direct Parquet reads are faster
5. ‚úÖ **Already have the data** - Traffic events flowing to HDFS via Kafka Connect

**What You Get:**
- ‚úÖ Query traffic data with SQL
- ‚úÖ Create training datasets
- ‚úÖ Train ML models
- ‚úÖ Real-time predictions
- ‚úÖ Dashboard integration (already done!)

**What You Don't Need:**
- ‚ùå Hive metastore complexity
- ‚ùå Broken Docker images
- ‚ùå Extra debugging time

## üìã Next Steps - ML Predictions Without Hive

### Phase 1: Spark Setup (30 minutes)
```powershell
# Spark containers already running!
docker ps --filter "name=spark"

# Test Spark
docker exec -it spark-master spark-submit --version
```

### Phase 2: Create PySpark Script (2 hours)
Create `src/batch-processing/feature_engineering.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window

spark = SparkSession.builder \
    .appName("Traffic Feature Engineering") \
    .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/warehouse") \
    .getOrCreate()

# Read traffic events from HDFS
df = spark.read.parquet("hdfs://namenode:9000/traffic/events")

# Feature engineering
features = df.withColumn("hour", hour("timestamp")) \
    .withColumn("day_of_week", dayofweek("timestamp")) \
    .groupBy("segment_id", "hour", "day_of_week") \
    .agg(
        avg("speed").alias("avg_speed"),
        stddev("speed").alias("speed_stddev"),
        avg("volume").alias("avg_volume"),
        count("*").alias("event_count")
    )

# Save features to HDFS
features.write.mode("overwrite").parquet("hdfs://namenode:9000/ml/features")

print("‚úÖ Feature engineering complete!")
```

### Phase 3: Train ML Models (4 hours)
Create `src/batch-processing/train_models.py`:

```python
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator

# Read features
features = spark.read.parquet("hdfs://namenode:9000/ml/features")

# Prepare features
assembler = VectorAssembler(
    inputCols=["hour", "day_of_week", "speed_stddev", "avg_volume"],
    outputCol="features"
)
data = assembler.transform(features)

# Split data
train, test = data.randomSplit([0.8, 0.2])

# Train Random Forest
rf = RandomForestRegressor(featuresCol="features", labelCol="avg_speed", numTrees=100)
rf_model = rf.fit(train)

# Evaluate
predictions = rf_model.transform(test)
evaluator = RegressionEvaluator(labelCol="avg_speed", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)

print(f"‚úÖ Random Forest RMSE: {rmse}")

# Save model to HDFS
rf_model.write().overwrite().save("hdfs://namenode:9000/ml/models/random_forest")
```

### Phase 4: Real-time Inference (2 hours)
Create `src/stream-processing/ml_predictor.py`:

```python
from pyspark.sql import SparkSession
from pyspark.ml.regression import RandomForestRegressionModel

spark = SparkSession.builder \
    .appName("Real-time Traffic Predictions") \
    .getOrCreate()

# Load trained model from HDFS
model = RandomForestRegressionModel.load("hdfs://namenode:9000/ml/models/random_forest")

# Read streaming data from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka-broker1:9092") \
    .option("subscribe", "traffic-events") \
    .load()

# Transform and predict
predictions = model.transform(df)

# Write predictions back to Kafka
predictions.selectExpr("CAST(prediction AS STRING) as value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka-broker1:9092") \
    .option("topic", "traffic-predictions") \
    .option("checkpointLocation", "hdfs://namenode:9000/checkpoints/predictions") \
    .start() \
    .awaitTermination()
```

## üöÄ Quick Start - Bypass Hive

```powershell
# 1. Stop Hive containers (save resources)
docker-compose stop hive-server hive-metastore

# 2. Test Spark is working
docker exec -it spark-master pyspark

# 3. In PySpark shell:
df = spark.read.parquet("hdfs://namenode:9000/traffic/events")
df.show(10)

# 4. Success! You can query HDFS data without Hive
```

## üìä Comparison: With vs Without Hive

| Feature | With Hive | Without Hive (Spark SQL) |
|---------|-----------|--------------------------|
| SQL Queries | ‚úÖ | ‚úÖ |
| Parquet Support | ‚úÖ | ‚úÖ |
| Partitioning | ‚úÖ | ‚úÖ |
| Performance | Medium | Fast |
| Setup Complexity | High | Low |
| Working in Docker | ‚ùå (broken) | ‚úÖ |
| Time to implement | 2+ days | 4 hours |

## üí° Conclusion

**The bde2020/hive Docker image is broken and not worth fixing.**

**Use Spark SQL directly - it's faster, simpler, and already working!**

Your real-time dashboard is already connected to Kafka. Just add Spark ML models and you're done!

## üìÅ Files Modified
- ‚úÖ docker-compose.yml - Updated Hive configuration (can revert)
- ‚úÖ fix-hive.ps1 - Created troubleshooting script
- ‚úÖ HIVE_FIX_DOCUMENTATION.md - Detailed fix attempts
- ‚úÖ THIS FILE - Recommendations to bypass Hive

## Next Actions

**Option A: Try different Hive image (1-2 hours)**
```powershell
# Update docker-compose.yml to use apache/hive:3.1.3
docker-compose up -d hive-metastore hive-server
```

**Option B: Skip Hive, use Spark SQL (RECOMMENDED - 4 hours total)**
```powershell
# Stop Hive
docker-compose stop hive-server hive-metastore

# Start implementing Spark ML pipeline
# Follow Phase 1-4 above
```

Your choice! But I strongly recommend **Option B** to save time and avoid Docker image headaches.
