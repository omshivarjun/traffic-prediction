# ðŸŒŠ Traffic Prediction System - Complete Data Flow & Workflow Documentation

> **Comprehensive guide documenting the end-to-end data flow and workflow for all system components**

**Last Updated:** October 7, 2025  
**System Version:** Production v1.0  
**Architecture:** Multi-layered Big Data Pipeline (Kafka + Hadoop + Spark + Next.js)

---

## ðŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Layers](#architecture-layers)
3. [Data Flow Diagrams](#data-flow-diagrams)
4. [Component Workflows](#component-workflows)
5. [Integration Points](#integration-points)
6. [Deployment Flow](#deployment-flow)

---

## ðŸŽ¯ System Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRAFFIC PREDICTION SYSTEM                          â”‚
â”‚                     Multi-Layered Big Data Pipeline                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA       â”‚      â”‚   STREAM     â”‚      â”‚    BATCH     â”‚
â”‚  INGESTION   â”‚â”€â”€â”€â”€â”€â–¶â”‚  PROCESSING  â”‚â”€â”€â”€â”€â”€â–¶â”‚  PROCESSING  â”‚
â”‚   LAYER      â”‚      â”‚    LAYER     â”‚      â”‚    LAYER     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚
       â–¼                     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE LAYER (HDFS + Kafka)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚
       â–¼                     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PREDICTION  â”‚      â”‚  ANALYTICS   â”‚      â”‚  FRONTEND    â”‚
â”‚   SERVICE    â”‚â—€â”€â”€â”€â”€â”€â”‚   SERVICE    â”‚â—€â”€â”€â”€â”€â”€â”‚  DASHBOARD   â”‚
â”‚   LAYER      â”‚      â”‚    LAYER     â”‚      â”‚    LAYER     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Technologies

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Ingestion** | Python Producer, Kafka Producer API | Ingest METR-LA traffic data |
| **Streaming** | Kafka, Kafka Streams (Node.js), Spark Streaming | Real-time event processing |
| **Batch** | Hadoop MapReduce, Spark MLlib | Feature engineering & ML training |
| **Storage** | HDFS, HBase, Kafka Topics | Persistent & distributed storage |
| **ML** | Spark MLlib, Python scikit-learn | Model training & predictions |
| **API** | Next.js API Routes, FastAPI | RESTful and SSE endpoints |
| **Frontend** | Next.js 15, React 19, Leaflet | Interactive dashboard |

---

## ðŸ—ï¸ Architecture Layers

### Layer 1: Data Ingestion

**Purpose:** Ingest raw traffic data from METR-LA dataset and external sources

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA INGESTION LAYER                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Sources:
  ðŸ“Š METR-LA Dataset (CSV)
  ðŸ“Š Test Scenarios (JSON)
  ðŸ“Š Real-time Sensors (Future)

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCERS                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ TrafficProducer    â”‚  â”‚ Test Event       â”‚                  â”‚
â”‚  â”‚ (Python)           â”‚  â”‚ Generator (PS)   â”‚                  â”‚
â”‚  â”‚                    â”‚  â”‚                  â”‚                  â”‚
â”‚  â”‚ â€¢ Reads CSV        â”‚  â”‚ â€¢ Generates      â”‚                  â”‚
â”‚  â”‚ â€¢ Parses METR-LA   â”‚  â”‚   mock events    â”‚                  â”‚
â”‚  â”‚ â€¢ Rate limiting    â”‚  â”‚ â€¢ For testing    â”‚                  â”‚
â”‚  â”‚ â€¢ Batching         â”‚  â”‚                  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KAFKA TOPICS (Raw Events)                                       â”‚
â”‚  â€¢ traffic-raw          : Raw unvalidated events                 â”‚
â”‚  â€¢ traffic-events       : Validated events                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Files:**
- `src/producer/traffic_producer.py` - Main producer implementation
- `scripts/kafka_producer.py` - Test event producer
- `scripts/send-test-events.ps1` - PowerShell test script
- `scripts/generate-metr-la-data.py` - METR-LA dataset loader

**Data Flow:**

1. **CSV Reading** â†’ Producer reads METR-LA CSV with 207 sensors
2. **Event Construction** â†’ Creates TrafficEvent objects with:
   - `event_id`: Unique identifier
   - `segment_id`: Road segment (LA_001 - LA_207)
   - `timestamp`: Unix timestamp (milliseconds)
   - `speed_kmh`: Speed in km/h
   - `volume_vph`: Vehicles per hour
   - `occupancy_percent`: Lane occupancy %
   - `coordinates`: Lat/Lon [latitude, longitude]
   - `metadata`: Additional context
3. **Kafka Publishing** â†’ Sends to `traffic-raw` topic
4. **Partitioning** â†’ Events partitioned by `segment_id` for parallelism

**Configuration:**
```ini
# producer_config.ini
[producer]
kafka_bootstrap_servers = localhost:9092
topic_name = traffic-raw
replay_speed = 1.0        # Real-time = 1.0
batch_size = 100
compression_type = snappy
acks = 1
```

---

### Layer 2: Stream Processing

**Purpose:** Real-time validation, transformation, and aggregation of traffic events

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAM PROCESSING LAYER                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kafka Topics (Input):
  â€¢ traffic-raw (26 partitions)

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STREAM PROCESSORS                                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Node.js Kafka Streams Processor                       â”‚    â”‚
â”‚  â”‚  (src/stream-processing/index.js)                      â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Consume from traffic-raw                           â”‚    â”‚
â”‚  â”‚  2. Validate event schema                              â”‚    â”‚
â”‚  â”‚  3. Check data quality                                 â”‚    â”‚
â”‚  â”‚  4. Flatten nested structures                          â”‚    â”‚
â”‚  â”‚  5. Add processing metadata                            â”‚    â”‚
â”‚  â”‚  6. Publish to traffic-events                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Spark Structured Streaming Processor                  â”‚    â”‚
â”‚  â”‚  (scripts/spark_streaming_processor.py)                â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Read from Kafka (traffic-events)                   â”‚    â”‚
â”‚  â”‚  2. Windowed aggregations (5-min tumbling)             â”‚    â”‚
â”‚  â”‚  3. Calculate statistics (avg, min, max, std)          â”‚    â”‚
â”‚  â”‚  4. Write to HDFS (Parquet)                            â”‚    â”‚
â”‚  â”‚  5. Maintain streaming state                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

Kafka Topics (Output):
  â€¢ traffic-events (12 partitions)        : Validated events
  â€¢ processed-traffic-aggregates (8)      : Aggregated data

HDFS Output:
  â€¢ /traffic-data/streaming/raw-events
  â€¢ /traffic-data/streaming/sensor-aggregates
  â€¢ /traffic-data/streaming/road-aggregates
```

**Stream Processing Workflows:**

#### Workflow 1: Event Validation & Transformation

```javascript
// Node.js Stream Processor (index.js)

Input Event (traffic-raw):
{
  "event_id": "evt_12345",
  "segment_id": "LA_123",
  "timestamp": 1696723200000,
  "speed_kmh": 65.5,
  "volume_vph": 450,
  "occupancy_percent": 12.3,
  "coordinates": [34.0522, -118.2437],
  "metadata": { "quality": "high" }
}

         â”‚
         â–¼

[Validation Layer]
âœ“ Schema validation (required fields present)
âœ“ Data type checks (speed > 0, volume >= 0)
âœ“ Range validation (0 <= occupancy <= 100)
âœ“ Coordinate bounds check

         â”‚
         â–¼

[Transformation Layer]
â€¢ Flatten nested structures
â€¢ Add processing timestamp
â€¢ Add processor_id
â€¢ Calculate derived fields

         â”‚
         â–¼

Output Event (traffic-events):
{
  "event_id": "evt_12345",
  "segment_id": "LA_123",
  "timestamp": 1696723200000,
  "speed_kmh": 65.5,
  "volume_vph": 450,
  "occupancy_percent": 12.3,
  "latitude": 34.0522,
  "longitude": -118.2437,
  "quality": "high",
  "processed_at": "2025-10-07T12:00:00.123Z",
  "processor_id": "stream-processor-1",
  "validation_status": "valid"
}
```

#### Workflow 2: Windowed Aggregation

```python
# Spark Streaming Processor (spark_streaming_processor.py)

[Read from Kafka]
  topic: traffic-events
  format: JSON
  
         â”‚
         â–¼

[Define Windows]
  â€¢ Tumbling window: 5 minutes
  â€¢ Watermark: 10 seconds (handle late data)
  
         â”‚
         â–¼

[Aggregation Operations]
  GROUP BY: segment_id, window
  METRICS:
    - avg_speed: AVG(speed_kmh)
    - min_speed: MIN(speed_kmh)
    - max_speed: MAX(speed_kmh)
    - std_speed: STDDEV(speed_kmh)
    - total_volume: SUM(volume_vph)
    - avg_occupancy: AVG(occupancy_percent)
    - event_count: COUNT(*)
  
         â”‚
         â–¼

[Output to HDFS]
  Path: /traffic-data/streaming/sensor-aggregates
  Format: Parquet
  Partitioning: year/month/day/hour
  Checkpoint: /tmp/spark-checkpoint/sensor-aggregates
```

**Key Files:**
- `src/stream-processing/index.js` - Node.js event processor
- `src/stream-processing/processors/` - Specialized processors
- `scripts/spark_streaming_processor.py` - Spark streaming job
- `src/validation/input_data_validator.py` - Validation logic

**Performance Metrics:**
- **Throughput:** ~1,000 events/second
- **Latency:** <100ms (validation) + <5s (aggregation)
- **Error Rate:** <0.1%

---

### Layer 3: Batch Processing

**Purpose:** Feature engineering and ML model training on historical data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BATCH PROCESSING LAYER                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HDFS Input:
  â€¢ /traffic-data/raw                    : Raw historical events
  â€¢ /traffic-data/streaming/aggregates   : Streaming aggregates

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEATURE ENGINEERING PIPELINE                                    â”‚
â”‚  (src/batch-processing/spark_feature_engineering.py)             â”‚
â”‚                                                                   â”‚
â”‚  Step 1: Load Data from HDFS                                     â”‚
â”‚    â†’ Read Parquet files                                          â”‚
â”‚    â†’ Schema validation                                           â”‚
â”‚                                                                   â”‚
â”‚  Step 2: Temporal Features                                       â”‚
â”‚    â†’ hour, day_of_week, day_of_month, month                      â”‚
â”‚    â†’ is_weekend, is_rush_hour                                    â”‚
â”‚    â†’ hour_sin, hour_cos (cyclical encoding)                      â”‚
â”‚                                                                   â”‚
â”‚  Step 3: Rolling Window Features                                 â”‚
â”‚    â†’ speed_rolling_avg (15min, 30min, 1hr)                       â”‚
â”‚    â†’ speed_rolling_std, speed_rolling_min, speed_rolling_max     â”‚
â”‚    â†’ volume_rolling_avg, volume_rolling_std                      â”‚
â”‚                                                                   â”‚
â”‚  Step 4: Lagged Features                                         â”‚
â”‚    â†’ speed_lag_15min, speed_lag_30min, speed_lag_60min           â”‚
â”‚    â†’ volume_lag_15min, volume_lag_30min                          â”‚
â”‚                                                                   â”‚
â”‚  Step 5: Segment-Level Features                                  â”‚
â”‚    â†’ segment_avg_speed, segment_std_speed                        â”‚
â”‚    â†’ segment_avg_volume, segment_std_volume                      â”‚
â”‚    â†’ segment_min_speed, segment_max_speed                        â”‚
â”‚                                                                   â”‚
â”‚  Step 6: Derived Features                                        â”‚
â”‚    â†’ speed_change = speed - speed_lag_15min                      â”‚
â”‚    â†’ speed_normalized = (speed - mean) / std                     â”‚
â”‚    â†’ congestion_index = volume / avg_volume                      â”‚
â”‚                                                                   â”‚
â”‚  Step 7: Save to HDFS                                            â”‚
â”‚    â†’ ml_features (Parquet, partitioned by segment_id)            â”‚
â”‚    â†’ segment_profiles (summary stats per segment)                â”‚
â”‚    â†’ hourly_aggregates (time-based patterns)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

HDFS Output:
  â€¢ /traffic-data/features/ml_features
  â€¢ /traffic-data/features/segment_profiles
  â€¢ /traffic-data/features/hourly_aggregates

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML MODEL TRAINING PIPELINE                                      â”‚
â”‚  (src/batch-processing/spark_train_models.py)                    â”‚
â”‚                                                                   â”‚
â”‚  Step 1: Load Features                                           â”‚
â”‚    â†’ Read from /traffic-data/features/ml_features                â”‚
â”‚    â†’ Schema validation                                           â”‚
â”‚                                                                   â”‚
â”‚  Step 2: Feature Preparation                                     â”‚
â”‚    â†’ Select feature columns (18 features)                        â”‚
â”‚    â†’ VectorAssembler: Create feature vectors                     â”‚
â”‚    â†’ StandardScaler: Normalize features                          â”‚
â”‚                                                                   â”‚
â”‚  Step 3: Train/Val/Test Split                                    â”‚
â”‚    â†’ Train: 70% (chronological split)                            â”‚
â”‚    â†’ Validation: 15%                                             â”‚
â”‚    â†’ Test: 15%                                                   â”‚
â”‚                                                                   â”‚
â”‚  Step 4: Model Training                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Linear Regression                       â”‚                  â”‚
â”‚    â”‚  â€¢ Fast baseline model                  â”‚                  â”‚
â”‚    â”‚  â€¢ Interpretable coefficients           â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Random Forest Regressor                 â”‚                  â”‚
â”‚    â”‚  â€¢ numTrees: 100                        â”‚                  â”‚
â”‚    â”‚  â€¢ maxDepth: 10                         â”‚                  â”‚
â”‚    â”‚  â€¢ Handles non-linearity                â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Gradient Boosted Trees                  â”‚                  â”‚
â”‚    â”‚  â€¢ maxIter: 100                         â”‚                  â”‚
â”‚    â”‚  â€¢ stepSize: 0.1                        â”‚                  â”‚
â”‚    â”‚  â€¢ Best accuracy                        â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Decision Tree Regressor                 â”‚                  â”‚
â”‚    â”‚  â€¢ maxDepth: 10                         â”‚                  â”‚
â”‚    â”‚  â€¢ Simple interpretable model           â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                   â”‚
â”‚  Step 5: Model Evaluation                                        â”‚
â”‚    â†’ RMSE (Root Mean Squared Error)                              â”‚
â”‚    â†’ RÂ² Score (Coefficient of Determination)                     â”‚
â”‚    â†’ MAE (Mean Absolute Error)                                   â”‚
â”‚    â†’ Select best model based on validation RMSE                  â”‚
â”‚                                                                   â”‚
â”‚  Step 6: Model Persistence                                       â”‚
â”‚    â†’ Save models to HDFS                                         â”‚
â”‚    â†’ Save preprocessing pipeline                                 â”‚
â”‚    â†’ Export model metadata (JSON)                                â”‚
â”‚    â†’ Generate model evaluation report                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

HDFS Output:
  â€¢ /traffic-data/models/linear_regression
  â€¢ /traffic-data/models/random_forest
  â€¢ /traffic-data/models/gradient_boosted_trees
  â€¢ /traffic-data/models/decision_tree
  â€¢ /traffic-data/models/preprocessing_pipeline
  â€¢ /traffic-data/models/metadata.json
```

**Batch Job Execution:**

```bash
# Submit Feature Engineering Job to YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 4 \
  --executor-memory 2G \
  --driver-memory 1G \
  src/batch-processing/spark_feature_engineering.py \
  hdfs://namenode:9000/traffic-data/raw \
  hdfs://namenode:9000/traffic-data/features

# Submit ML Training Job
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 4 \
  --executor-memory 2G \
  --driver-memory 1G \
  src/batch-processing/spark_train_models.py \
  hdfs://namenode:9000/traffic-data/features/ml_features \
  hdfs://namenode:9000/traffic-data/models
```

**Key Files:**
- `src/batch-processing/spark_feature_engineering.py` - Feature pipeline
- `src/batch-processing/spark_train_models.py` - ML training
- `src/batch-processing/train_models.py` - Alternative training script
- `src/feature_engineering/feature_pipeline.py` - Feature orchestrator
- `src/ml/ml_training_pipeline.py` - Complete ML pipeline

**Feature Engineering Output Schema:**

```python
ml_features Schema:
â”œâ”€â”€ segment_id: string
â”œâ”€â”€ timestamp: long
â”œâ”€â”€ speed: double
â”œâ”€â”€ volume: double
â”œâ”€â”€ occupancy: double
â”œâ”€â”€ hour: integer
â”œâ”€â”€ day_of_week: integer
â”œâ”€â”€ day_of_month: integer
â”œâ”€â”€ month: integer
â”œâ”€â”€ is_weekend: integer (0/1)
â”œâ”€â”€ is_rush_hour: integer (0/1)
â”œâ”€â”€ speed_rolling_avg: double
â”œâ”€â”€ speed_rolling_std: double
â”œâ”€â”€ speed_rolling_min: double
â”œâ”€â”€ speed_rolling_max: double
â”œâ”€â”€ speed_change: double
â”œâ”€â”€ speed_normalized: double
â”œâ”€â”€ segment_avg_speed: double
â”œâ”€â”€ segment_std_speed: double
â”œâ”€â”€ volume_rolling_avg: double
â”œâ”€â”€ volume_rolling_std: double
â”œâ”€â”€ segment_avg_volume: double
â””â”€â”€ segment_std_volume: double
```

---

### Layer 4: Storage Layer

**Purpose:** Persistent storage for raw data, features, models, and streaming state

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STORAGE LAYER                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HDFS (Hadoop 3.2)  â”‚         â”‚  Kafka Topics        â”‚
â”‚   Port: 9000         â”‚         â”‚  Port: 9092          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Directory Structure: â”‚         â”‚ Topics:              â”‚
â”‚                      â”‚         â”‚                      â”‚
â”‚ /traffic-data/       â”‚         â”‚ â€¢ traffic-raw        â”‚
â”‚   â”œâ”€â”€ raw/           â”‚         â”‚   (26 partitions)    â”‚
â”‚   â”‚   â””â”€â”€ year/      â”‚         â”‚                      â”‚
â”‚   â”‚       â””â”€â”€ month/ â”‚         â”‚ â€¢ traffic-events     â”‚
â”‚   â”‚           â””â”€â”€ day/â”‚        â”‚   (12 partitions)    â”‚
â”‚   â”‚                  â”‚         â”‚                      â”‚
â”‚   â”œâ”€â”€ streaming/     â”‚         â”‚ â€¢ processed-traffic- â”‚
â”‚   â”‚   â”œâ”€â”€ raw-events/â”‚         â”‚   aggregates         â”‚
â”‚   â”‚   â”œâ”€â”€ sensor-    â”‚         â”‚   (8 partitions)     â”‚
â”‚   â”‚   â”‚   aggregates/â”‚         â”‚                      â”‚
â”‚   â”‚   â””â”€â”€ road-      â”‚         â”‚ â€¢ traffic-predictionsâ”‚
â”‚   â”‚       aggregates/â”‚         â”‚   (4 partitions)     â”‚
â”‚   â”‚                  â”‚         â”‚                      â”‚
â”‚   â”œâ”€â”€ features/      â”‚         â”‚ â€¢ traffic-incidents  â”‚
â”‚   â”‚   â”œâ”€â”€ ml_featuresâ”‚         â”‚   (4 partitions)     â”‚
â”‚   â”‚   â”œâ”€â”€ segment_   â”‚         â”‚                      â”‚
â”‚   â”‚   â”‚   profiles/  â”‚         â”‚ Retention:           â”‚
â”‚   â”‚   â””â”€â”€ hourly_    â”‚         â”‚ â€¢ 7 days (default)   â”‚
â”‚   â”‚       aggregates/â”‚         â”‚                      â”‚
â”‚   â”‚                  â”‚         â”‚ Compression:         â”‚
â”‚   â””â”€â”€ models/        â”‚         â”‚ â€¢ Snappy             â”‚
â”‚       â”œâ”€â”€ linear_    â”‚         â”‚                      â”‚
â”‚       â”‚   regression/â”‚         â”‚ Replication:         â”‚
â”‚       â”œâ”€â”€ random_    â”‚         â”‚ â€¢ Factor: 1          â”‚
â”‚       â”‚   forest/    â”‚         â”‚                      â”‚
â”‚       â”œâ”€â”€ gradient_  â”‚         â”‚ Consumer Groups:     â”‚
â”‚       â”‚   boosted_   â”‚         â”‚ â€¢ stream-processor   â”‚
â”‚       â”‚   trees/     â”‚         â”‚ â€¢ prediction-service â”‚
â”‚       â”œâ”€â”€ preprocessingâ”‚        â”‚ â€¢ nextjs-dashboard   â”‚
â”‚       â””â”€â”€ metadata.  â”‚         â”‚                      â”‚
â”‚           json       â”‚         â”‚                      â”‚
â”‚                      â”‚         â”‚                      â”‚
â”‚ Format: Parquet      â”‚         â”‚ Format: Avro/JSON    â”‚
â”‚ Compression: Snappy  â”‚         â”‚ Schema Registry:     â”‚
â”‚ Replication: 3       â”‚         â”‚ â€¢ Port 8081          â”‚
â”‚ Block Size: 128MB    â”‚         â”‚ â€¢ Avro schemas       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HBase (Optional)   â”‚
â”‚   Port: 16010        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tables:              â”‚
â”‚                      â”‚
â”‚ â€¢ traffic_events     â”‚
â”‚   Row Key: segment_idâ”‚
â”‚            + timestampâ”‚
â”‚                      â”‚
â”‚ â€¢ predictions        â”‚
â”‚   Row Key: segment_idâ”‚
â”‚            + timestampâ”‚
â”‚                      â”‚
â”‚ Column Families:     â”‚
â”‚ â€¢ data (sensor data) â”‚
â”‚ â€¢ meta (metadata)    â”‚
â”‚                      â”‚
â”‚ Use Case:            â”‚
â”‚ â€¢ Real-time lookups  â”‚
â”‚ â€¢ Fast key-value     â”‚
â”‚   access             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**HDFS Directory Details:**

| Path | Purpose | Format | Partitioning |
|------|---------|--------|-------------|
| `/traffic-data/raw` | Raw historical events | Parquet | year/month/day |
| `/traffic-data/streaming/raw-events` | Real-time events from Kafka | Parquet | year/month/day/hour |
| `/traffic-data/streaming/sensor-aggregates` | 5-min aggregated sensor data | Parquet | year/month/day |
| `/traffic-data/features/ml_features` | ML-ready features | Parquet | segment_id |
| `/traffic-data/features/segment_profiles` | Statistical profiles per segment | Parquet | - |
| `/traffic-data/models/` | Trained ML models | Spark ML | model_type |

**Kafka Topic Configuration:**

```yaml
Topics:
  traffic-raw:
    partitions: 26
    replication-factor: 1
    retention.ms: 604800000  # 7 days
    compression.type: snappy
    
  traffic-events:
    partitions: 12
    replication-factor: 1
    retention.ms: 604800000
    compression.type: snappy
    
  processed-traffic-aggregates:
    partitions: 8
    replication-factor: 1
    retention.ms: 2592000000  # 30 days
    
  traffic-predictions:
    partitions: 4
    replication-factor: 1
    retention.ms: 604800000
    cleanup.policy: delete
```

---

### Layer 5: Prediction Service

**Purpose:** Real-time traffic prediction using trained ML models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PREDICTION SERVICE LAYER                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Sources:
  1. Kafka: traffic-events topic
  2. HDFS: Trained models
  3. HBase: Historical patterns (optional)

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STREAMING PREDICTION SERVICE                                    â”‚
â”‚  (src/ml/simple_streaming_predictor.py)                          â”‚
â”‚                                                                   â”‚
â”‚  Initialization:                                                 â”‚
â”‚    1. Load trained model from HDFS                               â”‚
â”‚    2. Load preprocessing pipeline                                â”‚
â”‚    3. Connect to Kafka (consumer + producer)                     â”‚
â”‚    4. Initialize feature cache                                   â”‚
â”‚                                                                   â”‚
â”‚  Real-Time Processing Loop:                                      â”‚
â”‚                                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 1. Consume Event from Kafka          â”‚                     â”‚
â”‚    â”‚    Topic: traffic-events              â”‚                     â”‚
â”‚    â”‚    Event: { segment_id, timestamp,    â”‚                     â”‚
â”‚    â”‚            speed, volume, ... }       â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 2. Feature Extraction                 â”‚                     â”‚
â”‚    â”‚    â€¢ Extract temporal features        â”‚                     â”‚
â”‚    â”‚    â€¢ Get historical patterns          â”‚                     â”‚
â”‚    â”‚    â€¢ Calculate rolling averages       â”‚                     â”‚
â”‚    â”‚    â€¢ Retrieve segment statistics      â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 3. Feature Vector Assembly            â”‚                     â”‚
â”‚    â”‚    Features:                          â”‚                     â”‚
â”‚    â”‚    [hour, day_of_week, is_weekend,    â”‚                     â”‚
â”‚    â”‚     is_rush_hour, speed_rolling_avg,  â”‚                     â”‚
â”‚    â”‚     speed_rolling_std, volume_avg,    â”‚                     â”‚
â”‚    â”‚     segment_avg_speed, ...]           â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 4. Model Prediction                   â”‚                     â”‚
â”‚    â”‚    predicted_speed_15min = model.     â”‚                     â”‚
â”‚    â”‚                   predict(features)   â”‚                     â”‚
â”‚    â”‚    predicted_speed_30min = model.     â”‚                     â”‚
â”‚    â”‚                   predict(features2)  â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 5. Post-Processing                    â”‚                     â”‚
â”‚    â”‚    â€¢ Clamp predictions (0-120 mph)    â”‚                     â”‚
â”‚    â”‚    â€¢ Calculate confidence intervals   â”‚                     â”‚
â”‚    â”‚    â€¢ Assign congestion category:      â”‚                     â”‚
â”‚    â”‚      - free_flow (>50 mph)            â”‚                     â”‚
â”‚    â”‚      - light_traffic (35-50)          â”‚                     â”‚
â”‚    â”‚      - heavy_traffic (20-35)          â”‚                     â”‚
â”‚    â”‚      - severe_congestion (<20)        â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 6. Create Prediction Event            â”‚                     â”‚
â”‚    â”‚    {                                  â”‚                     â”‚
â”‚    â”‚      prediction_id,                   â”‚                     â”‚
â”‚    â”‚      segment_id,                      â”‚                     â”‚
â”‚    â”‚      timestamp,                       â”‚                     â”‚
â”‚    â”‚      current_speed,                   â”‚                     â”‚
â”‚    â”‚      current_volume,                  â”‚                     â”‚
â”‚    â”‚      predicted_speed,                 â”‚                     â”‚
â”‚    â”‚      predicted_volume,                â”‚                     â”‚
â”‚    â”‚      prediction_horizon_minutes: 15,  â”‚                     â”‚
â”‚    â”‚      confidence_score,                â”‚                     â”‚
â”‚    â”‚      category,                        â”‚                     â”‚
â”‚    â”‚      coordinates: [lat, lon],         â”‚                     â”‚
â”‚    â”‚      model_version                    â”‚                     â”‚
â”‚    â”‚    }                                  â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                                    â”‚
â”‚              â–¼                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚    â”‚ 7. Publish to Kafka                   â”‚                     â”‚
â”‚    â”‚    Topic: traffic-predictions         â”‚                     â”‚
â”‚    â”‚    Partition Key: segment_id          â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

Kafka Topic (Output):
  â€¢ traffic-predictions (4 partitions)

         â”‚
         â–¼

Consumers:
  â€¢ Next.js Dashboard (via predictionConsumer.ts)
  â€¢ Analytics Service
  â€¢ Alerting System
```

**Prediction Event Schema:**

```json
{
  "prediction_id": "pred_abc123",
  "segment_id": "LA_123",
  "timestamp": 1696723200000,
  "prediction_timestamp": 1696724100000,
  "current_speed": 45.5,
  "current_volume": 450,
  "predicted_speed": 38.2,
  "predicted_volume": 520,
  "prediction_horizon_minutes": 15,
  "confidence_score": 0.87,
  "confidence_interval": {
    "lower": 35.1,
    "upper": 41.3
  },
  "category": "heavy_traffic",
  "coordinates": [34.0522, -118.2437],
  "model_version": "random_forest_v1.2",
  "features_used": [...],
  "processing_time_ms": 45
}
```

**Key Files:**
- `src/ml/simple_streaming_predictor.py` - Main prediction service
- `src/prediction/prediction_service.py` - Alternative implementation
- `src/ml/streaming_predictor.py` - Advanced predictor
- `src/prediction/realtime_prediction_service.py` - Real-time service

---

### Layer 6: Frontend Dashboard

**Purpose:** Interactive web dashboard for visualizing predictions and analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FRONTEND DASHBOARD LAYER                    â”‚
â”‚                    Next.js 15 + React 19 + Leaflet              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Access:
  http://localhost:3000/predictions

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEXT.JS APP ROUTER                                              â”‚
â”‚  (src/app/)                                                      â”‚
â”‚                                                                   â”‚
â”‚  Pages:                                                          â”‚
â”‚  â€¢ /                    â†’ Home page                              â”‚
â”‚  â€¢ /predictions         â†’ Main predictions dashboard             â”‚
â”‚  â€¢ /city-planner        â†’ City planner tools                     â”‚
â”‚                                                                   â”‚
â”‚  API Routes:                                                     â”‚
â”‚  â€¢ GET  /api/predictions        â†’ Fetch latest predictions       â”‚
â”‚  â€¢ GET  /api/predictions/stream â†’ SSE real-time stream           â”‚
â”‚  â€¢ POST /api/predictions        â†’ Receive prediction (webhook)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKEND API LAYER (Next.js API Routes)                          â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ GET /api/predictions/stream                            â”‚     â”‚
â”‚  â”‚ (src/app/api/predictions/stream/route.ts)              â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Server-Sent Events Endpoint                            â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ 1. Get singleton Kafka consumer                        â”‚     â”‚
â”‚  â”‚ 2. Start consumer if not running                       â”‚     â”‚
â”‚  â”‚    â†’ Connect to Kafka broker                           â”‚     â”‚
â”‚  â”‚    â†’ Wait 1 second for broker initialization           â”‚     â”‚
â”‚  â”‚    â†’ Subscribe to 'traffic-predictions' topic          â”‚     â”‚
â”‚  â”‚ 3. Create SSE stream                                   â”‚     â”‚
â”‚  â”‚ 4. Send initial connection message                     â”‚     â”‚
â”‚  â”‚ 5. Send current predictions (initial state)            â”‚     â”‚
â”‚  â”‚ 6. Subscribe to new prediction events                  â”‚     â”‚
â”‚  â”‚ 7. Stream predictions to client in real-time           â”‚     â”‚
â”‚  â”‚ 8. Send stats every 10 seconds                         â”‚     â”‚
â”‚  â”‚ 9. Handle client disconnect (cleanup)                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Kafka Consumer (Singleton)                             â”‚     â”‚
â”‚  â”‚ (src/lib/kafka/predictionConsumer.ts)                  â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Responsibilities:                                      â”‚     â”‚
â”‚  â”‚ â€¢ Connect to Kafka broker (localhost:9092)             â”‚     â”‚
â”‚  â”‚ â€¢ Subscribe to traffic-predictions topic               â”‚     â”‚
â”‚  â”‚ â€¢ Consume prediction messages                          â”‚     â”‚
â”‚  â”‚ â€¢ Maintain latest predictions Map (by segment_id)      â”‚     â”‚
â”‚  â”‚ â€¢ Calculate statistics (total, avg speed, categories)  â”‚     â”‚
â”‚  â”‚ â€¢ Notify subscribers on new predictions                â”‚     â”‚
â”‚  â”‚ â€¢ Handle reconnection and errors                       â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Methods:                                               â”‚     â”‚
â”‚  â”‚ â€¢ start() - Start consuming                            â”‚     â”‚
â”‚  â”‚ â€¢ stop() - Stop and disconnect                         â”‚     â”‚
â”‚  â”‚ â€¢ getAllPredictions() - Get current state              â”‚     â”‚
â”‚  â”‚ â€¢ getStats() - Get aggregated stats                    â”‚     â”‚
â”‚  â”‚ â€¢ onPrediction(callback) - Subscribe to updates        â”‚     â”‚
â”‚  â”‚ â€¢ isRunning() - Check consumer status                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚
         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REACT FRONTEND COMPONENTS                                       â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ PredictionsPage                                        â”‚     â”‚
â”‚  â”‚ (src/app/predictions/page.tsx)                         â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Main Dashboard Container:                              â”‚     â”‚
â”‚  â”‚ â€¢ Uses usePredictions hook                             â”‚     â”‚
â”‚  â”‚ â€¢ Renders TrafficMapWithPredictions                    â”‚     â”‚
â”‚  â”‚ â€¢ Renders PredictionAnalyticsPanel                     â”‚     â”‚
â”‚  â”‚ â€¢ Shows connection status                              â”‚     â”‚
â”‚  â”‚ â€¢ Displays error messages                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ usePredictions Hook                                    â”‚     â”‚
â”‚  â”‚ (src/hooks/usePredictions.ts)                          â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ SSE Connection Manager:                                â”‚     â”‚
â”‚  â”‚ 1. Create EventSource to /api/predictions/stream       â”‚     â”‚
â”‚  â”‚ 2. Handle connection events                            â”‚     â”‚
â”‚  â”‚ 3. Process SSE messages:                               â”‚     â”‚
â”‚  â”‚    â€¢ 'connected' â†’ Set connection status               â”‚     â”‚
â”‚  â”‚    â€¢ 'initial' â†’ Load initial predictions              â”‚     â”‚
â”‚  â”‚    â€¢ 'prediction' â†’ Add new prediction                 â”‚     â”‚
â”‚  â”‚    â€¢ 'stats' â†’ Update statistics                       â”‚     â”‚
â”‚  â”‚    â€¢ 'error' â†’ Display error                           â”‚     â”‚
â”‚  â”‚ 4. Auto-reconnect on disconnect (max 5 attempts)       â”‚     â”‚
â”‚  â”‚ 5. Cleanup on unmount                                  â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ State Management:                                      â”‚     â”‚
â”‚  â”‚ â€¢ predictions: TrafficPrediction[]                     â”‚     â”‚
â”‚  â”‚ â€¢ stats: PredictionStats                               â”‚     â”‚
â”‚  â”‚ â€¢ isConnected: boolean                                 â”‚     â”‚
â”‚  â”‚ â€¢ error: string | null                                 â”‚     â”‚
â”‚  â”‚ â€¢ lastUpdate: Date | null                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ TrafficMapWithPredictions                              â”‚     â”‚
â”‚  â”‚ (src/components/TrafficMapWithPredictions.tsx)         â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Interactive Leaflet Map:                               â”‚     â”‚
â”‚  â”‚ â€¢ Display LA region (34.0522, -118.2437)               â”‚     â”‚
â”‚  â”‚ â€¢ Render prediction markers                            â”‚     â”‚
â”‚  â”‚ â€¢ Color-coded by congestion category:                  â”‚     â”‚
â”‚  â”‚   - Green: Free flow (>50 mph)                         â”‚     â”‚
â”‚  â”‚   - Yellow: Light traffic (35-50 mph)                  â”‚     â”‚
â”‚  â”‚   - Orange: Heavy traffic (20-35 mph)                  â”‚     â”‚
â”‚  â”‚   - Red: Severe congestion (<20 mph)                   â”‚     â”‚
â”‚  â”‚ â€¢ Click marker â†’ Show prediction details               â”‚     â”‚
â”‚  â”‚ â€¢ Real-time updates (predictions prop)                 â”‚     â”‚
â”‚  â”‚ â€¢ Auto-zoom to fit all markers                         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ PredictionAnalyticsPanel                               â”‚     â”‚
â”‚  â”‚ (src/components/PredictionAnalyticsPanel.tsx)          â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚ Real-Time Statistics Dashboard:                        â”‚     â”‚
â”‚  â”‚ â€¢ Total predictions count                              â”‚     â”‚
â”‚  â”‚ â€¢ Average predicted speed                              â”‚     â”‚
â”‚  â”‚ â€¢ Average current speed                                â”‚     â”‚
â”‚  â”‚ â€¢ Congestion breakdown (pie chart)                     â”‚     â”‚
â”‚  â”‚ â€¢ Recent predictions list                              â”‚     â”‚
â”‚  â”‚ â€¢ Last update timestamp                                â”‚     â”‚
â”‚  â”‚ â€¢ Connection status indicator                          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SSE Message Flow:**

```javascript
// Client-side (usePredictions.ts)

1. Create EventSource connection:
   const eventSource = new EventSource('/api/predictions/stream')

2. Handle messages:
   eventSource.onmessage = (event) => {
     const message = JSON.parse(event.data)
     
     switch(message.type) {
       case 'connected':
         // Set isConnected = true
         break
         
       case 'initial':
         // Load initial predictions
         setPredictions(message.predictions)
         break
         
       case 'prediction':
         // Add new prediction
         setPredictions(prev => [message.data, ...prev].slice(0, 50))
         setLastUpdate(new Date())
         break
         
       case 'stats':
         // Update statistics
         setStats(message.data)
         break
         
       case 'error':
         // Display error
         setError(message.message)
         break
     }
   }

3. Handle errors and reconnection:
   eventSource.onerror = () => {
     setIsConnected(false)
     // Attempt reconnect (max 5 times)
     if (reconnectAttempts < 5) {
       setTimeout(() => connect(), 3000)
     }
   }

4. Cleanup on unmount:
   return () => eventSource.close()
```

**Component Hierarchy:**

```
App (layout.tsx)
 â”‚
 â”œâ”€ HomePage (page.tsx)
 â”‚
 â””â”€ PredictionsPage (predictions/page.tsx)
     â”‚
     â”œâ”€ usePredictions() hook
     â”‚   â””â”€ EventSource â†’ /api/predictions/stream
     â”‚       â””â”€ Kafka Consumer â†’ traffic-predictions topic
     â”‚
     â”œâ”€ TrafficMapWithPredictions
     â”‚   â”œâ”€ Leaflet Map
     â”‚   â”œâ”€ TileLayer (OpenStreetMap)
     â”‚   â””â”€ Markers (color-coded by category)
     â”‚
     â””â”€ PredictionAnalyticsPanel
         â”œâ”€ Statistics Cards
         â”œâ”€ Congestion Chart
         â””â”€ Recent Predictions List
```

**Key Files:**
- `src/app/predictions/page.tsx` - Main dashboard page
- `src/hooks/usePredictions.ts` - SSE hook
- `src/components/TrafficMapWithPredictions.tsx` - Map component
- `src/components/PredictionAnalyticsPanel.tsx` - Analytics panel
- `src/lib/kafka/predictionConsumer.ts` - Kafka consumer singleton
- `src/app/api/predictions/stream/route.ts` - SSE endpoint
- `src/app/api/predictions/route.ts` - REST endpoint

---

## ðŸ”„ Complete End-to-End Data Flow

### E2E Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COMPLETE END-TO-END DATA FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1. DATA INGESTION]
  METR-LA CSV (207 sensors Ã— 12,672 timestamps)
         â”‚
         â–¼
  TrafficProducer.py
    â€¢ Parses CSV
    â€¢ Creates TrafficEvent objects
    â€¢ Sends to Kafka
         â”‚
         â–¼
  Kafka Topic: traffic-raw (26 partitions)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[2. STREAM PROCESSING - VALIDATION]
  Kafka Consumer (Node.js)
         â”‚
         â–¼
  ValidationProcessor
    â€¢ Schema validation
    â€¢ Data quality checks
    â€¢ Range validation
         â”‚
         â–¼
  TransformProcessor
    â€¢ Flatten structures
    â€¢ Add metadata
    â€¢ Calculate derived fields
         â”‚
         â–¼
  Kafka Topic: traffic-events (12 partitions)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[3. STREAM PROCESSING - AGGREGATION]
  Kafka Consumer (Spark Streaming)
         â”‚
         â–¼
  WindowedAggregationProcessor
    â€¢ 5-minute tumbling windows
    â€¢ Calculate statistics (avg, min, max, std)
    â€¢ Group by segment_id
         â”‚
         â–¼
  HDFS: /traffic-data/streaming/sensor-aggregates (Parquet)
  Kafka Topic: processed-traffic-aggregates (8 partitions)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[4. BATCH PROCESSING - FEATURE ENGINEERING]
  HDFS Input: /traffic-data/raw
         â”‚
         â–¼
  Spark Job: FeatureEngineeringJob
    â€¢ Load historical data
    â€¢ Extract temporal features
    â€¢ Create rolling window features
    â€¢ Calculate lagged features
    â€¢ Generate segment statistics
         â”‚
         â–¼
  HDFS Output:
    â€¢ /traffic-data/features/ml_features
    â€¢ /traffic-data/features/segment_profiles
    â€¢ /traffic-data/features/hourly_aggregates

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[5. BATCH PROCESSING - ML TRAINING]
  HDFS Input: /traffic-data/features/ml_features
         â”‚
         â–¼
  Spark Job: MLTrainingJob
    â€¢ Load features
    â€¢ Prepare training data (VectorAssembler + Scaler)
    â€¢ Split: Train (70%) / Val (15%) / Test (15%)
    â€¢ Train 4 models:
      - Linear Regression
      - Random Forest
      - Gradient Boosted Trees
      - Decision Tree
    â€¢ Evaluate (RMSE, RÂ², MAE)
    â€¢ Select best model
         â”‚
         â–¼
  HDFS Output:
    â€¢ /traffic-data/models/random_forest (best model)
    â€¢ /traffic-data/models/preprocessing_pipeline
    â€¢ /traffic-data/models/metadata.json

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[6. REAL-TIME PREDICTION]
  Kafka Topic: traffic-events
         â”‚
         â–¼
  StreamingPredictorService
    â€¢ Load model from HDFS
    â€¢ Consume events
    â€¢ Extract features
    â€¢ Make predictions
    â€¢ Post-process results
         â”‚
         â–¼
  Kafka Topic: traffic-predictions (4 partitions)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[7. FRONTEND DELIVERY]
  Kafka Topic: traffic-predictions
         â”‚
         â–¼
  Next.js Kafka Consumer (predictionConsumer.ts)
    â€¢ Consume predictions
    â€¢ Maintain latest state
    â€¢ Calculate statistics
         â”‚
         â–¼
  SSE Stream (/api/predictions/stream)
    â€¢ Stream to connected clients
    â€¢ Send stats every 10s
         â”‚
         â–¼
  React Frontend (usePredictions hook)
    â€¢ EventSource connection
    â€¢ Real-time updates
         â”‚
         â–¼
  Dashboard Components
    â€¢ TrafficMapWithPredictions (Leaflet map)
    â€¢ PredictionAnalyticsPanel (stats)
         â”‚
         â–¼
  User Browser
    â€¢ Visual map with color-coded markers
    â€¢ Real-time congestion updates
    â€¢ Analytics dashboard
```

### Timing & Latency

| Stage | Latency | Throughput |
|-------|---------|------------|
| **Data Ingestion** | ~10ms | 1,000 events/sec |
| **Stream Validation** | ~50ms | 800 events/sec |
| **Stream Aggregation** | ~5s (window) | 500 events/sec |
| **Batch Feature Engineering** | ~10-30 min | N/A (batch) |
| **Batch ML Training** | ~1-2 hours | N/A (batch) |
| **Real-Time Prediction** | ~100ms | 200 predictions/sec |
| **Frontend SSE Delivery** | ~50ms | Real-time |
| **End-to-End (ingestion â†’ dashboard)** | **~6 seconds** | - |

---

## ðŸ”— Integration Points

### 1. Kafka â†” Stream Processor

**Technology:** KafkaJS (Node.js), Kafka Consumer API (Python)

**Connection:**
```javascript
// Node.js Stream Processor
const kafka = new Kafka({
  clientId: 'stream-processor-client',
  brokers: ['kafka-broker1:9092'],
  retry: { initialRetryTime: 300, retries: 10 },
  connectionTimeout: 10000
})

const consumer = kafka.consumer({ groupId: 'stream-processor-group' })
await consumer.connect()
await consumer.subscribe({ topic: 'traffic-raw' })
```

**Data Exchange:**
- **Input:** Raw traffic events (JSON)
- **Output:** Validated events (JSON)
- **Error Handling:** Invalid events sent to `invalid-events` topic

---

### 2. Kafka â†” Spark Streaming

**Technology:** Spark Structured Streaming, Kafka Source

**Connection:**
```python
# Spark Streaming Job
df = spark.readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "kafka-broker1:9092") \
  .option("subscribe", "traffic-events") \
  .option("startingOffsets", "latest") \
  .load()
```

**Data Exchange:**
- **Input:** Validated traffic events (JSON)
- **Output:** 
  - HDFS: Parquet files
  - Kafka: `processed-traffic-aggregates` topic
- **Checkpointing:** `/tmp/spark-checkpoint/sensor-aggregates`

---

### 3. HDFS â†” Spark Batch Jobs

**Technology:** Spark SQL, Parquet

**Connection:**
```python
# Feature Engineering Job
df = spark.read.parquet("hdfs://namenode:9000/traffic-data/raw")

# ML Training Job
features_df = spark.read.parquet("hdfs://namenode:9000/traffic-data/features/ml_features")
```

**Data Exchange:**
- **Read:** Parquet files (compressed with Snappy)
- **Write:** Partitioned Parquet files
- **Schema Evolution:** Supported via Parquet metadata

---

### 4. HDFS â†” Prediction Service

**Technology:** Spark MLlib Model Persistence

**Connection:**
```python
from pyspark.ml import PipelineModel

# Load model
model = PipelineModel.load("hdfs://namenode:9000/traffic-data/models/random_forest")

# Load preprocessing pipeline
preprocessor = PipelineModel.load("hdfs://namenode:9000/traffic-data/models/preprocessing_pipeline")
```

**Data Exchange:**
- **Model Format:** Spark ML model (directory structure)
- **Metadata:** JSON file with model version, metrics
- **Versioning:** Timestamp-based directories

---

### 5. Kafka â†” Next.js Dashboard

**Technology:** KafkaJS, Server-Sent Events (SSE)

**Connection:**
```typescript
// predictionConsumer.ts
const kafka = new Kafka({
  clientId: 'nextjs-dashboard',
  brokers: ['localhost:9092'],
  retry: { retries: 8, initialRetryTime: 300 }
})

const consumer = kafka.consumer({
  groupId: 'nextjs-prediction-consumer-group',
  sessionTimeout: 60000
})

await consumer.connect()
await new Promise(resolve => setTimeout(resolve, 1000)) // Wait for broker
await consumer.subscribe({ topic: 'traffic-predictions' })
```

**Data Exchange:**
- **Input:** Traffic predictions (JSON)
- **Output:** SSE stream to browser
- **Error Handling:** Reconnection with exponential backoff

---

## ðŸš€ Deployment Flow

### Development Environment

```bash
# 1. Start Docker Infrastructure
docker compose up -d

# 2. Verify Services
docker ps
# Expected: Kafka, Zookeeper, HDFS NameNode/DataNode, YARN ResourceManager

# 3. Start Stream Processor
cd src/stream-processing
npm install
npm start

# 4. Start Prediction Service (optional)
cd src/ml
python simple_streaming_predictor.py

# 5. Start Next.js Dashboard
npm install
npm run dev

# 6. Generate Test Data
powershell -File .\scripts\send-test-events.ps1 -Count 100

# 7. Access Dashboard
open http://localhost:3000/predictions
```

### Production Deployment

```bash
# 1. Build Frontend
npm run build
npm start  # Production server

# 2. Submit Batch Jobs to YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 4 \
  --executor-memory 2G \
  src/batch-processing/spark_feature_engineering.py

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 4 \
  --executor-memory 2G \
  src/batch-processing/spark_train_models.py

# 3. Deploy Stream Processor (Docker)
docker build -t stream-processor src/stream-processing
docker run -d --name stream-processor \
  --network kafka-network \
  stream-processor

# 4. Start Prediction Service
python src/ml/simple_streaming_predictor.py \
  --kafka-broker kafka-broker1:9092 \
  --model-path hdfs://namenode:9000/traffic-data/models/random_forest

# 5. Monitor Services
# Kafka UI: http://localhost:8080
# HDFS UI: http://localhost:9870
# YARN UI: http://localhost:8088
# Next.js: http://localhost:3000
```

---

## ðŸ“Š Monitoring & Observability

### Key Metrics

| Component | Metrics | Dashboard |
|-----------|---------|-----------|
| **Kafka** | Messages/sec, Lag, Partition distribution | Kafka UI (port 8080) |
| **Stream Processor** | Throughput, Error rate, Latency | Console logs, Health endpoint |
| **Spark Jobs** | Task duration, Memory usage, Shuffle metrics | YARN UI (port 8088) |
| **HDFS** | Storage usage, Block replication, DataNode health | HDFS UI (port 9870) |
| **Prediction Service** | Predictions/sec, Model latency, Accuracy | Application logs |
| **Next.js** | Active connections, SSE clients, Prediction count | Browser console |

### Health Checks

```bash
# Kafka broker health
docker exec kafka-broker1 kafka-broker-api-versions --bootstrap-server localhost:9092

# HDFS health
docker exec namenode hdfs dfsadmin -report

# Stream processor health
curl http://localhost:3001/health

# Prediction service health
curl http://localhost:8000/health

# Next.js health
curl http://localhost:3000/api/health
```

---

## ðŸŽ“ Conclusion

This comprehensive documentation covers the complete data flow and workflow for all components in the Traffic Prediction System. Each layer is designed for:

- **Scalability:** Kafka partitioning, Spark distributed processing
- **Reliability:** Error handling, retry logic, checkpointing
- **Performance:** Batching, compression, caching
- **Maintainability:** Modular design, clear interfaces
- **Observability:** Logging, metrics, health checks

For component-specific details, refer to individual documentation files in the `docs/` directory.

---

## ðŸ”§ Troubleshooting

### Common Issues

#### 1. Multiple Kafka Consumer Instances (Rebalancing Errors)

**Symptoms:**
```
{"level":"ERROR","logger":"kafkajs","error":"The group is rebalancing, so a rejoin is needed"}
ðŸ”Œ Starting prediction consumer for SSE stream...
âš ï¸ Consumer already exists, waiting for connection...
```

**Cause:** Multiple Next.js processes or hot-reload creating duplicate consumers in the same consumer group.

**Solution:**
```powershell
# Kill all Node.js processes
Get-Process -Name node -ErrorAction SilentlyContinue | Stop-Process -Force

# Restart Next.js cleanly
npm run dev
```

**Prevention:** The singleton pattern in `predictionConsumer.ts` helps, but during development restarts, ensure only one Next.js instance is running.

---

#### 2. SSE Connection Drops After 60 Seconds

**Symptoms:**
```
GET /api/predictions/stream 200 in 60436ms
ðŸ“¡ Client disconnected from prediction stream
```

**Cause:** Browser EventSource timeout or network proxy closing idle connections.

**Solution:**
- Stats messages sent every 10 seconds keep connection alive
- Client auto-reconnects (5 attempts with 3s delay)
- Check for network proxies or VPN timeouts

---

#### 3. No Predictions Appearing on Dashboard

**Symptoms:**
- Dashboard shows "Connected! Waiting for predictions..."
- No markers on map

**Cause:** No events in `traffic-predictions` topic.

**Solution:**
```powershell
# Send test events
.\scripts\send-test-events.ps1 -Count 10

# Check if prediction service is running
docker ps | findstr prediction

# Verify Kafka topics
docker exec kafka-broker1 kafka-topics --list --bootstrap-server localhost:9092
```

---

#### 4. 404 Errors for `/predictions/stream`

**Symptoms:**
```
GET /predictions/stream 404 in 67ms
```

**Cause:** Incorrect endpoint (should be `/api/predictions/stream`).

**Solution:**
- The correct SSE endpoint is `/api/predictions/stream` (note the `/api/` prefix)
- Check `usePredictions.ts` hook is using correct path
- Clear browser cache if old routes are cached

---

#### 5. Kafka Consumer Not Starting

**Symptoms:**
```
âŒ Failed to connect to Kafka broker
Error: Broker not connected
```

**Cause:** Kafka broker not running or connection timeout.

**Solution:**
```powershell
# Check Kafka containers
docker ps | findstr kafka

# Restart Kafka services
docker compose restart kafka-broker1 zookeeper

# Verify Kafka health
docker exec kafka-broker1 kafka-broker-api-versions --bootstrap-server localhost:9092
```

---

## ðŸ“ž Support & Contact

For issues, questions, or contributions:
- **GitHub Issues:** [traffic-prediction/issues](https://github.com/omshivarjun/traffic-prediction/issues)
- **Documentation:** See other files in `docs/` directory
- **Team Structure:** Refer to `AGENTS.md`

---

**Document Version:** 1.0  
**Last Updated:** October 7, 2025  
**Maintained By:** Traffic Prediction Team  
**Contact:** See `AGENTS.md` for team structure
