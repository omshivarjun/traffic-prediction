"""
Spatial Feature Extraction Module

Extracts spatial/geographic features from sensor location data.
Includes highway encoding and normalized coordinates for LA area.

Task 3.2: Spatial Feature Extraction
"""

from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col, when, coalesce, lit
)
from pyspark.ml.feature import StringIndexer, OneHotEncoder


class SpatialFeatureExtractor:
    """
    Extract spatial features from sensor metadata.
    
    Features generated:
    - highway_encoded: One-hot encoded highway identifier
    - latitude_norm: Normalized latitude (0-1 scale for LA area)
    - longitude_norm: Normalized longitude (0-1 scale for LA area)
    
    Normalization bounds for Los Angeles metropolitan area:
    - Latitude: 33.7째 to 34.35째 N
    - Longitude: -118.7째 to -118.1째 W
    """
    
    # Los Angeles area bounds for normalization
    LAT_MIN = 33.7
    LAT_MAX = 34.35
    LON_MIN = -118.7
    LON_MAX = -118.1
    
    def __init__(self, 
                 sensor_id_col: str = "segment_id",
                 highway_col: str = "highway",
                 latitude_col: str = "latitude", 
                 longitude_col: str = "longitude"):
        """
        Initialize the spatial feature extractor.
        
        Args:
            sensor_id_col: Name of sensor/segment ID column
            highway_col: Name of highway identifier column
            latitude_col: Name of latitude column
            longitude_col: Name of longitude column
        """
        self.sensor_id_col = sensor_id_col
        self.highway_col = highway_col
        self.latitude_col = latitude_col
        self.longitude_col = longitude_col
        
    def extract_features(self, df: DataFrame) -> DataFrame:
        """
        Extract spatial features from the DataFrame.
        
        Args:
            df: Spark DataFrame with sensor location data
            
        Returns:
            DataFrame with additional spatial feature columns
            
        Features:
        - latitude_norm: (latitude - LAT_MIN) / (LAT_MAX - LAT_MIN)
        - longitude_norm: (longitude - LON_MIN) / (LON_MAX - LON_MIN)
        - highway_I5, highway_I10, highway_I110, highway_I405, highway_US101: 
          One-hot encoded highway identifiers
        """
        # Normalize latitude (0-1 scale)
        df = df.withColumn(
            "latitude_norm",
            (col(self.latitude_col) - lit(self.LAT_MIN)) / 
            (lit(self.LAT_MAX) - lit(self.LAT_MIN))
        )
        
        # Normalize longitude (0-1 scale)
        df = df.withColumn(
            "longitude_norm",
            (col(self.longitude_col) - lit(self.LON_MIN)) / 
            (lit(self.LON_MAX) - lit(self.LON_MIN))
        )
        
        # One-hot encode highway
        # Common LA highways: I-5, I-10, I-110, I-405, US-101
        # Create binary columns for each major highway
        
        highways = ["I-5", "I-10", "I-110", "I-405", "US-101"]
        
        for hwy in highways:
            # Create safe column name (replace hyphens with underscores)
            col_name = f"highway_{hwy.replace('-', '_')}"
            df = df.withColumn(
                col_name,
                when(col(self.highway_col) == hwy, 1).otherwise(0)
            )
        
        # Create "other" category for highways not in the main list
        df = df.withColumn(
            "highway_other",
            when(
                ~col(self.highway_col).isin(highways),
                1
            ).otherwise(0)
        )
        
        return df
    
    def get_feature_names(self) -> list:
        """
        Get the list of feature names generated by this extractor.
        
        Returns:
            List of feature column names
        """
        return [
            "latitude_norm",
            "longitude_norm",
            "highway_I_5",
            "highway_I_10",
            "highway_I_110",
            "highway_I_405",
            "highway_US_101",
            "highway_other"
        ]


# Example usage
if __name__ == "__main__":
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType
    
    # Create Spark session
    spark = SparkSession.builder \
        .appName("SpatialFeatureExtractorTest") \
        .getOrCreate()
    
    # Create sample data
    schema = StructType([
        StructField("segment_id", StringType(), False),
        StructField("highway", StringType(), False),
        StructField("latitude", DoubleType(), False),
        StructField("longitude", DoubleType(), False),
        StructField("speed", DoubleType(), False)
    ])
    
    data = [
        ("LA_001", "I-5", 34.0522, -118.2437, 55.0),
        ("LA_002", "I-10", 34.0195, -118.4912, 62.5),
        ("LA_003", "I-110", 33.7866, -118.2987, 48.3),
        ("LA_004", "I-405", 34.0689, -118.4452, 52.1),
        ("LA_005", "US-101", 34.1478, -118.1445, 58.9),
        ("LA_006", "CA-60", 34.0368, -117.9389, 60.2),  # Other highway
    ]
    
    df = spark.createDataFrame(data, schema)
    
    # Extract features
    extractor = SpatialFeatureExtractor()
    df_with_features = extractor.extract_features(df)
    
    # Show results
    print("\nSpatial Features Extracted:")
    df_with_features.select(
        "segment_id", "highway",
        "latitude_norm", "longitude_norm",
        "highway_I_5", "highway_I_10", "highway_I_110",
        "highway_I_405", "highway_US_101", "highway_other"
    ).show(truncate=False)
    
    print("\nFeature Names:")
    print(extractor.get_feature_names())
    
    print("\nNormalization Info:")
    print(f"Latitude range: {extractor.LAT_MIN} to {extractor.LAT_MAX}")
    print(f"Longitude range: {extractor.LON_MIN} to {extractor.LON_MAX}")
    
    spark.stop()
