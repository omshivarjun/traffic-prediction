"""
Historical Feature Extraction Module

Extracts lag features and rolling statistics from time series traffic data.
Provides temporal context for prediction models.

Task 3.4: Historical Feature Extraction
"""

from pyspark.sql import DataFrame, Window
from pyspark.sql.functions import (
    col, lag, avg, stddev, min as spark_min, max as spark_max
)


class HistoricalFeatureExtractor:
    """
    Extract historical/temporal features from traffic time series.
    
    Features generated:
    - Lag features: Previous 1, 2, 3 time periods
    - Rolling statistics: Mean, std dev over windows (3, 6, 12 periods)
    - Min/max over recent windows
    
    These features provide temporal context critical for time series prediction.
    """
    
    def __init__(self,
                 timestamp_col: str = "timestamp",
                 sensor_id_col: str = "segment_id",
                 target_cols: list = None):
        """
        Initialize the historical feature extractor.
        
        Args:
            timestamp_col: Name of timestamp column for ordering
            sensor_id_col: Name of sensor/segment ID column for grouping
            target_cols: List of columns to create lag/rolling features for
                        (default: ["speed", "volume", "occupancy"])
        """
        self.timestamp_col = timestamp_col
        self.sensor_id_col = sensor_id_col
        self.target_cols = target_cols or ["speed", "volume", "occupancy"]
        
    def extract_features(self, df: DataFrame) -> DataFrame:
        """
        Extract historical features from the DataFrame.
        
        Args:
            df: Spark DataFrame with time series traffic data
            
        Returns:
            DataFrame with additional historical feature columns
            
        Features for each target column:
        - {col}_lag_1, {col}_lag_2, {col}_lag_3: Previous 1-3 periods
        - {col}_rolling_mean_3, {col}_rolling_mean_6: Rolling mean over 3, 6 periods
        - {col}_rolling_std_3: Rolling std dev over 3 periods
        - {col}_rolling_min_6, {col}_rolling_max_6: Rolling min/max over 6 periods
        """
        # Define window for lag features (partitioned by sensor, ordered by time)
        window_spec = Window.partitionBy(self.sensor_id_col).orderBy(self.timestamp_col)
        
        # Define rolling windows
        window_3 = window_spec.rowsBetween(-2, 0)  # Current + previous 2 rows
        window_6 = window_spec.rowsBetween(-5, 0)  # Current + previous 5 rows
        window_12 = window_spec.rowsBetween(-11, 0)  # Current + previous 11 rows
        
        result_df = df
        
        for target_col in self.target_cols:
            if target_col not in df.columns:
                continue
                
            # Lag features (1, 2, 3 periods back)
            result_df = result_df.withColumn(
                f"{target_col}_lag_1",
                lag(col(target_col), 1).over(window_spec)
            )
            result_df = result_df.withColumn(
                f"{target_col}_lag_2",
                lag(col(target_col), 2).over(window_spec)
            )
            result_df = result_df.withColumn(
                f"{target_col}_lag_3",
                lag(col(target_col), 3).over(window_spec)
            )
            
            # Rolling mean over 3 periods
            result_df = result_df.withColumn(
                f"{target_col}_rolling_mean_3",
                avg(col(target_col)).over(window_3)
            )
            
            # Rolling mean over 6 periods
            result_df = result_df.withColumn(
                f"{target_col}_rolling_mean_6",
                avg(col(target_col)).over(window_6)
            )
            
            # Rolling mean over 12 periods (hourly if 5-min aggregates)
            result_df = result_df.withColumn(
                f"{target_col}_rolling_mean_12",
                avg(col(target_col)).over(window_12)
            )
            
            # Rolling standard deviation over 3 periods
            result_df = result_df.withColumn(
                f"{target_col}_rolling_std_3",
                stddev(col(target_col)).over(window_3)
            )
            
            # Rolling min/max over 6 periods
            result_df = result_df.withColumn(
                f"{target_col}_rolling_min_6",
                spark_min(col(target_col)).over(window_6)
            )
            result_df = result_df.withColumn(
                f"{target_col}_rolling_max_6",
                spark_max(col(target_col)).over(window_6)
            )
        
        return result_df
    
    def get_feature_names(self) -> list:
        """
        Get the list of feature names generated by this extractor.
        
        Returns:
            List of feature column names
        """
        features = []
        for target_col in self.target_cols:
            features.extend([
                f"{target_col}_lag_1",
                f"{target_col}_lag_2",
                f"{target_col}_lag_3",
                f"{target_col}_rolling_mean_3",
                f"{target_col}_rolling_mean_6",
                f"{target_col}_rolling_mean_12",
                f"{target_col}_rolling_std_3",
                f"{target_col}_rolling_min_6",
                f"{target_col}_rolling_max_6"
            ])
        return features


# Example usage
if __name__ == "__main__":
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
    from datetime import datetime, timedelta
    
    # Create Spark session
    spark = SparkSession.builder \
        .appName("HistoricalFeatureExtractorTest") \
        .getOrCreate()
    
    # Create sample time series data (5-minute intervals)
    schema = StructType([
        StructField("segment_id", StringType(), False),
        StructField("timestamp", TimestampType(), False),
        StructField("speed", DoubleType(), False),
        StructField("volume", DoubleType(), False),
        StructField("occupancy", DoubleType(), False)
    ])
    
    base_time = datetime(2025, 1, 6, 8, 0, 0)
    data = []
    
    # Generate 15 records for sensor LA_001 (5-minute intervals over 1 hour 15 min)
    speeds = [62, 58, 55, 52, 48, 45, 42, 38, 35, 40, 45, 50, 55, 58, 60]
    for i, speed in enumerate(speeds):
        data.append((
            "LA_001",
            base_time + timedelta(minutes=i*5),
            float(speed),
            1500.0 + (i * 50),
            0.3 + (i * 0.02)
        ))
    
    df = spark.createDataFrame(data, schema)
    
    # Extract features
    extractor = HistoricalFeatureExtractor(target_cols=["speed"])
    df_with_features = extractor.extract_features(df)
    
    # Show results
    print("\nHistorical Features Extracted:")
    print("=" * 100)
    df_with_features.select(
        "segment_id", "timestamp", "speed",
        "speed_lag_1", "speed_lag_2", "speed_lag_3",
        "speed_rolling_mean_3", "speed_rolling_mean_6",
        "speed_rolling_std_3",
        "speed_rolling_min_6", "speed_rolling_max_6"
    ).show(15, truncate=False)
    
    print("\nFeature Names (for 'speed' only):")
    print([f for f in extractor.get_feature_names() if f.startswith("speed")])
    
    print("\nNote: Lag and rolling features will be NULL for initial rows")
    print("This is expected and will be handled during model training (e.g., dropna)")
    
    spark.stop()
