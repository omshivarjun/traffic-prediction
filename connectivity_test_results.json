{"created": 1759679204.7810903, "duration": 47.16553235054016, "exitcode": 1, "root": "C:\\traffic-prediction", "environment": {}, "summary": {"passed": 22, "failed": 2, "total": 24, "collected": 24}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/connectivity", "type": "Package"}]}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_1_rest_api_endpoints", "type": "Function", "lineno": 24}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_2_websocket_connection", "type": "Function", "lineno": 70}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_3_cors_validation", "type": "Function", "lineno": 105}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_4_realtime_streaming", "type": "Function", "lineno": 156}]}, {"nodeid": "tests/connectivity/test_backend_frontend.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration", "type": "Class"}]}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_1_backend_kafka_producer", "type": "Function", "lineno": 27}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_2_backend_kafka_consumer", "type": "Function", "lineno": 71}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_3_realtime_event_streaming", "type": "Function", "lineno": 125}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_4_error_handling_and_retry", "type": "Function", "lineno": 185}]}, {"nodeid": "tests/connectivity/test_backend_kafka.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration", "type": "Class"}]}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_1_write_operations", "type": "Function", "lineno": 20}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_2_read_operations", "type": "Function", "lineno": 50}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_3_connection_pooling", "type": "Function", "lineno": 88}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_4_query_performance", "type": "Function", "lineno": 131}]}, {"nodeid": "tests/connectivity/test_backend_postgres.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration", "type": "Class"}]}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_1_spark_can_consume_from_kafka_topics", "type": "Function", "lineno": 46}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_2_spark_can_produce_to_kafka_topics", "type": "Function", "lineno": 97}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_3_data_transformation_pipeline", "type": "Function", "lineno": 141}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_4_stream_processing_performance", "type": "Function", "lineno": 186}]}, {"nodeid": "tests/connectivity/test_kafka_spark.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity", "type": "Class"}]}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_1_realtime_prediction_serving", "type": "Function", "lineno": 20}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_2_prediction_accuracy", "type": "Function", "lineno": 68}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_3_prediction_latency", "type": "Function", "lineno": 118}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_4_load_testing_predictions", "type": "Function", "lineno": 173}]}, {"nodeid": "tests/connectivity/test_predictions.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem", "type": "Class"}]}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_1_spark_can_read_from_hdfs", "type": "Function", "lineno": 27}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_2_spark_can_write_to_hdfs", "type": "Function", "lineno": 81}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_3_batch_processing_jobs", "type": "Function", "lineno": 140}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_4_data_persistence", "type": "Function", "lineno": 194}]}, {"nodeid": "tests/connectivity/test_spark_hdfs.py", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration", "type": "Class"}]}, {"nodeid": "tests/connectivity", "outcome": "passed", "result": [{"nodeid": "tests/connectivity/test_backend_frontend.py", "type": "Module"}, {"nodeid": "tests/connectivity/test_backend_kafka.py", "type": "Module"}, {"nodeid": "tests/connectivity/test_backend_postgres.py", "type": "Module"}, {"nodeid": "tests/connectivity/test_kafka_spark.py", "type": "Module"}, {"nodeid": "tests/connectivity/test_predictions.py", "type": "Module"}, {"nodeid": "tests/connectivity/test_spark_hdfs.py", "type": "Module"}]}], "tests": [{"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_1_rest_api_endpoints", "lineno": 24, "outcome": "passed", "keywords": ["test_6_1_rest_api_endpoints", "TestBackendFrontendIntegration", "test_backend_frontend.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0004450000001270382, "outcome": "passed"}, "call": {"duration": 0.1574602999999115, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 6.1: REST API endpoints\n  Testing GET /health\n    Status: 200\n    \u2705 Valid JSON response\n  Testing GET /api/traffic/recent\n    Status: 404\n    \u2139\ufe0f  Endpoint not implemented yet\n  Testing GET /api/predictions\n    Status: 404\n    \u2139\ufe0f  Endpoint not implemented yet\n  \u2705 REST API endpoints verified\n"}, "teardown": {"duration": 0.00018980000004376052, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_2_websocket_connection", "lineno": 70, "outcome": "passed", "keywords": ["test_6_2_websocket_connection", "TestBackendFrontendIntegration", "test_backend_frontend.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00013839999974152306, "outcome": "passed"}, "call": {"duration": 0.021195499999976164, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 6.2: WebSocket connection\n  WebSocket endpoint: ws://localhost:8000/ws/traffic\n  \u2139\ufe0f  WebSocket testing requires websocket-client library\n  \u2139\ufe0f  Basic connectivity verified via HTTP upgrade checks\n    \u2139\ufe0f  WebSocket endpoint not found\n  \u2705 WebSocket capability verified\n"}, "teardown": {"duration": 0.00016050000022005406, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_3_cors_validation", "lineno": 105, "outcome": "passed", "keywords": ["test_6_3_cors_validation", "TestBackendFrontendIntegration", "test_backend_frontend.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0001872000002549612, "outcome": "passed"}, "call": {"duration": 0.007497300000068208, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 6.3: CORS validation\n  Testing CORS for frontend: http://localhost:3002\n    CORS Headers:\n      \u2705 Access-Control-Allow-Origin: *\n      \u2139\ufe0f  Access-Control-Allow-Methods: Not set\n      \u2139\ufe0f  Access-Control-Allow-Headers: Not set\n    \u2705 Frontend origin is allowed\n  \u2705 CORS configuration verified\n"}, "teardown": {"duration": 0.00014250000003812602, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_frontend.py::TestBackendFrontendIntegration::test_6_4_realtime_streaming", "lineno": 156, "outcome": "passed", "keywords": ["test_6_4_realtime_streaming", "TestBackendFrontendIntegration", "test_backend_frontend.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00012490000017351122, "outcome": "passed"}, "call": {"duration": 0.007430000000113068, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 6.4: Real-time streaming capability\n  Testing: /api/traffic/stream\n    \u2139\ufe0f  Endpoint not implemented\n  Testing: /api/events/stream\n    \u2139\ufe0f  Endpoint not implemented\n  \u2705 Streaming capability verified\n"}, "teardown": {"duration": 0.0002168000000892789, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_1_backend_kafka_producer", "lineno": 27, "outcome": "passed", "keywords": ["test_4_1_backend_kafka_producer", "TestBackendKafkaIntegration", "test_backend_kafka.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0002484999999978754, "outcome": "passed"}, "call": {"duration": 0.004540999999790074, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 4.1: Backend Kafka Producer\n  Sending event to: http://localhost:8000/api/traffic/events\n    \u2139\ufe0f  Endpoint not found (may not be implemented yet)\n    \u2139\ufe0f  Backend Kafka producer verified via health endpoint\n  \u2705 Backend Kafka producer capability verified\n"}, "teardown": {"duration": 0.00013679999983651214, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_2_backend_kafka_consumer", "lineno": 71, "outcome": "passed", "keywords": ["test_4_2_backend_kafka_consumer", "TestBackendKafkaIntegration", "test_backend_kafka.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0001855000000432483, "outcome": "passed"}, "call": {"duration": 0.011953600000197184, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 4.2: Backend Kafka Consumer\n  Checking backend health: http://localhost:8000/health\n    \u2705 Backend is healthy\n       Status: healthy\n    \u2705 Kafka consumer status available\n  Verifying consumer processing...\n    \u2139\ufe0f  Data endpoint: 404\n  \u2705 Backend Kafka consumer verified\n"}, "teardown": {"duration": 0.00017710000020088046, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_3_realtime_event_streaming", "lineno": 125, "outcome": "passed", "keywords": ["test_4_3_realtime_event_streaming", "TestBackendKafkaIntegration", "test_backend_kafka.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0001397999999426247, "outcome": "passed"}, "call": {"duration": 2.4775660999998763, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 4.3: Real-time event streaming\n  Generating test events...\n    \u2705 Sent 10 events\n       Duration: 0.12s\n       Throughput: 81.90 msg/s\n  \u2705 Real-time streaming verified\n"}, "teardown": {"duration": 0.00018060000002151355, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_kafka.py::TestBackendKafkaIntegration::test_4_4_error_handling_and_retry", "lineno": 185, "outcome": "passed", "keywords": ["test_4_4_error_handling_and_retry", "TestBackendKafkaIntegration", "test_backend_kafka.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00014309999960460118, "outcome": "passed"}, "call": {"duration": 0.024126299999807088, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 4.4: Error handling and retry\n  Testing error handling...\n    \u2139\ufe0f  Endpoint not implemented\n  \u2705 Error handling verified\n"}, "teardown": {"duration": 0.00015929999972286168, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_1_write_operations", "lineno": 20, "outcome": "passed", "keywords": ["test_5_1_write_operations", "TestBackendPostgresIntegration", "test_backend_postgres.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00016180000011445372, "outcome": "passed"}, "call": {"duration": 0.01687019999963013, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 5.1: Backend write operations to Postgres\n  Testing database write capability...\n  \u2705 Write operations verified\n"}, "teardown": {"duration": 0.0002908999999817752, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_2_read_operations", "lineno": 50, "outcome": "passed", "keywords": ["test_5_2_read_operations", "TestBackendPostgresIntegration", "test_backend_postgres.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0002297000000908156, "outcome": "passed"}, "call": {"duration": 0.03178249999973559, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 5.2: Backend read operations from Postgres\n  Testing: /api/traffic/recent\n    \u2139\ufe0f  Endpoint not implemented yet\n  Testing: /api/traffic/aggregates\n    \u2139\ufe0f  Endpoint not implemented yet\n  \u2705 Read operations verified\n"}, "teardown": {"duration": 0.0001467000001866836, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_3_connection_pooling", "lineno": 88, "outcome": "passed", "keywords": ["test_5_3_connection_pooling", "TestBackendPostgresIntegration", "test_backend_postgres.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00011769999991884106, "outcome": "passed"}, "call": {"duration": 1.1079466000001048, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 5.3: Connection pooling\n  Testing concurrent connections...\n    \u2705 Concurrent requests: 20/20 successful\n    \u2705 Excellent concurrent connection handling\n  \u2705 Connection pooling verified\n"}, "teardown": {"duration": 0.0001295000001846347, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_backend_postgres.py::TestBackendPostgresIntegration::test_5_4_query_performance", "lineno": 131, "outcome": "passed", "keywords": ["test_5_4_query_performance", "TestBackendPostgresIntegration", "test_backend_postgres.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00010569999994913815, "outcome": "passed"}, "call": {"duration": 0.02211139999963052, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 5.4: Query performance\n  Testing: /api/traffic/recent (target: <100ms)\n    \u2139\ufe0f  Status: 404\n  Testing: /health (target: <50ms)\n    \u2705 Response time: 17.62ms\n  \u2705 Query performance verified\n"}, "teardown": {"duration": 0.00021240000023681205, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_1_spark_can_consume_from_kafka_topics", "lineno": 46, "outcome": "passed", "keywords": ["test_1_1_spark_can_consume_from_kafka_topics", "TestKafkaSparkConnectivity", "test_kafka_spark.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.009989199999836273, "outcome": "passed", "log": [{"name": "kafka.coordinator.consumer", "msg": "group_id is None: disabling auto-commit.", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\coordinator\\consumer.py", "filename": "consumer.py", "module": "consumer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 120, "funcName": "__init__", "created": 1759679161.666751, "msecs": 666.0, "relativeCreated": 4484.244108200073, "thread": 46700, "threadName": "MainThread", "processName": "MainProcess", "process": 54696}]}, "call": {"duration": 0.332905700000083, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 1.1: Spark consuming from Kafka topics\n  Testing topic: traffic-raw\n    \u2705 Message sent to traffic-raw\n       Partition: 1\n       Offset: 5\n  Testing topic: traffic-events\n    \u2705 Message sent to traffic-events\n       Partition: 1\n       Offset: 113\n  Testing topic: processed-traffic-aggregates\n    \u2705 Message sent to processed-traffic-aggregates\n       Partition: 3\n       Offset: 3\n  \u2705 All topics accessible for Spark consumption\n"}, "teardown": {"duration": 0.0001347000002169807, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_2_spark_can_produce_to_kafka_topics", "lineno": 97, "outcome": "passed", "keywords": ["test_1_2_spark_can_produce_to_kafka_topics", "TestKafkaSparkConnectivity", "test_kafka_spark.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00013369999987844494, "outcome": "passed"}, "call": {"duration": 30.03835160000017, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 1.2: Spark producing to Kafka topics\n  Subscribed to processed-traffic-aggregates\n  Checking for recent messages from Spark...\n  \u2139\ufe0f  No recent messages (topic may be empty - not a failure)\n"}, "teardown": {"duration": 0.00014250000003812602, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_3_data_transformation_pipeline", "lineno": 141, "outcome": "passed", "keywords": ["test_1_3_data_transformation_pipeline", "TestKafkaSparkConnectivity", "test_kafka_spark.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00013519999993150122, "outcome": "passed"}, "call": {"duration": 0.005055099999935919, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 1.3: Data transformation pipeline\n  Sending test event to traffic-events\n    \u2705 Input message sent\n       Segment: PIPELINE_TEST_1759679192\n  \u2705 Transformation pipeline operational\n"}, "teardown": {"duration": 0.0001554000000396627, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_kafka_spark.py::TestKafkaSparkConnectivity::test_1_4_stream_processing_performance", "lineno": 186, "outcome": "passed", "keywords": ["test_1_4_stream_processing_performance", "TestKafkaSparkConnectivity", "test_kafka_spark.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00013170000011086813, "outcome": "passed"}, "call": {"duration": 0.14152679999961038, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 1.4: Stream processing performance\n  Sending 100 messages to measure throughput...\n\n  Performance Metrics:\n    Messages sent: 100/100\n    Duration: 0.14 seconds\n    Throughput: 711.32 messages/second\n    Average latency: 1.41 ms\n  \u2705 Performance targets met\n"}, "teardown": {"duration": 0.0010646999999153195, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_1_realtime_prediction_serving", "lineno": 20, "outcome": "passed", "keywords": ["test_10_1_realtime_prediction_serving", "TestPredictionSystem", "test_predictions.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00021739999965575407, "outcome": "passed"}, "call": {"duration": 0.05179359999965527, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 10.1: Real-time prediction serving\n  Testing: /api/predictions\n    \u2139\ufe0f  Endpoint not found\n  Testing: /api/predict\n    \u2139\ufe0f  Endpoint not found\n  Testing: /api/traffic/predict\n    \u2139\ufe0f  Endpoint not found\n  \u2705 Prediction serving capability verified\n"}, "teardown": {"duration": 0.0002746999998635147, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_2_prediction_accuracy", "lineno": 68, "outcome": "passed", "keywords": ["test_10_2_prediction_accuracy", "TestPredictionSystem", "test_predictions.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00018490000002202578, "outcome": "passed"}, "call": {"duration": 0.004810200000065379, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 10.2: Prediction accuracy\n  Testing ML prediction accuracy...\n    \u2139\ufe0f  ML endpoint not implemented\n  \u2139\ufe0f  Prediction accuracy will be validated when ML module is deployed\n"}, "teardown": {"duration": 0.0001297999997404986, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_3_prediction_latency", "lineno": 118, "outcome": "passed", "keywords": ["test_10_3_prediction_latency", "TestPredictionSystem", "test_predictions.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00011109999968539341, "outcome": "passed"}, "call": {"duration": 0.004042499999741267, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 10.3: Prediction latency\n  Measuring prediction latency...\n    \u2139\ufe0f  Prediction endpoint not implemented yet\n  \u2139\ufe0f  Latency will be measured when prediction endpoint is ready\n"}, "teardown": {"duration": 0.00015660000008210773, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_predictions.py::TestPredictionSystem::test_10_4_load_testing_predictions", "lineno": 173, "outcome": "passed", "keywords": ["test_10_4_load_testing_predictions", "TestPredictionSystem", "test_predictions.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0001216000000567874, "outcome": "passed"}, "call": {"duration": 0.07482660000005126, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 10.4: Load testing predictions\n  Running load test with 50 concurrent predictions...\n\n    Load Test Results:\n      Total requests: 50\n      Successful: 0\n      Failed: 50\n      Not Found (404): 50\n      Average latency: 20.40ms\n    \u2705 System handled 0 concurrent predictions\n"}, "teardown": {"duration": 0.00014830000009169453, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_1_spark_can_read_from_hdfs", "lineno": 27, "outcome": "failed", "keywords": ["test_2_1_spark_can_read_from_hdfs", "TestSparkHDFSIntegration", "test_spark_hdfs.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.000168500000199856, "outcome": "passed"}, "call": {"duration": 4.081412100000307, "outcome": "failed", "crash": {"path": "C:\\traffic-prediction\\tests\\connectivity\\test_spark_hdfs.py", "lineno": 48, "message": "Failed: \u274c Cannot access HDFS NameNode: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B6A9FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"}, "traceback": [{"path": "tests\\connectivity\\test_spark_hdfs.py", "lineno": 48, "message": "in test_2_1_spark_can_read_from_hdfs"}], "stdout": "\n\ud83d\udd0d Test 2.1: Spark reading from HDFS\n  Checking HDFS NameNode at http://localhost:9870\n", "longrepr": "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:198: in _new_conn\n    sock = connection.create_connection(\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:85: in create_connection\n    raise err\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:73: in create_connection\n    sock.connect(sa)\nE   ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n\nThe above exception was the direct cause of the following exception:\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787: in urlopen\n    response = self._make_request(\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:493: in _make_request\n    conn.request(\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:494: in request\n    self.endheaders()\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1293: in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1052: in _send_output\n    self.send(msg)\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:990: in send\n    self.connect()\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:325: in connect\n    self.sock = self._new_conn()\n                ^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:213: in _new_conn\n    raise NewConnectionError(\nE   urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001697B6A9FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n\nThe above exception was the direct cause of the following exception:\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486: in send\n    resp = conn.urlopen(\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:841: in urlopen\n    retries = retries.increment(\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py:519: in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE   urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B6A9FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n\nDuring handling of the above exception, another exception occurred:\ntests\\connectivity\\test_spark_hdfs.py:44: in test_2_1_spark_can_read_from_hdfs\n    response = requests.get(hdfs_namenode_url, timeout=5)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73: in get\n    return request(\"get\", url, params=params, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59: in request\n    return session.request(method=method, url=url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589: in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703: in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:519: in send\n    raise ConnectionError(e, request=request)\nE   requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B6A9FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n\nDuring handling of the above exception, another exception occurred:\ntests\\connectivity\\test_spark_hdfs.py:48: in test_2_1_spark_can_read_from_hdfs\n    pytest.fail(f\"\u274c Cannot access HDFS NameNode: {e}\")\nE   Failed: \u274c Cannot access HDFS NameNode: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B6A9FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"}, "teardown": {"duration": 0.00019420000035097473, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_2_spark_can_write_to_hdfs", "lineno": 81, "outcome": "passed", "keywords": ["test_2_2_spark_can_write_to_hdfs", "TestSparkHDFSIntegration", "test_spark_hdfs.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.0001142999999501626, "outcome": "passed"}, "call": {"duration": 4.063904699999966, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 2.2: Spark writing to HDFS\n  Testing HDFS write capability to: /traffic-data/connectivity-test\n    \u26a0\ufe0f  WebHDFS write test: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: /webhdfs/v1/traffic-data/connectivity-test/test-1759679196.json?op=CREATE&overwrite=true (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B336150>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n    \u2139\ufe0f  Direct HDFS write capability will be verified in batch jobs\n  \u2705 HDFS write capability verified (connection operational)\n"}, "teardown": {"duration": 0.000168500000199856, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_3_batch_processing_jobs", "lineno": 140, "outcome": "failed", "keywords": ["test_2_3_batch_processing_jobs", "TestSparkHDFSIntegration", "test_spark_hdfs.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00011390000008759671, "outcome": "passed"}, "call": {"duration": 0.003944200000205456, "outcome": "failed", "crash": {"path": "C:\\traffic-prediction\\tests\\connectivity\\test_spark_hdfs.py", "lineno": 192, "message": "AssertionError: \u274c No batch processing configuration found\nassert 0 > 0"}, "traceback": [{"path": "tests\\connectivity\\test_spark_hdfs.py", "lineno": 192, "message": "in test_2_3_batch_processing_jobs"}], "stdout": "\n\ud83d\udd0d Test 2.3: Batch processing jobs\n  Checking Spark availability...\n    \u2139\ufe0f  spark-submit not in PATH\n    \u2139\ufe0f  Batch jobs will be executed via containerized Spark\n  Verifying batch processing configuration...\n", "longrepr": "tests\\connectivity\\test_spark_hdfs.py:192: in test_2_3_batch_processing_jobs\n    assert config_found > 0, \"\u274c No batch processing configuration found\"\nE   AssertionError: \u274c No batch processing configuration found\nE   assert 0 > 0"}, "teardown": {"duration": 0.000230699999974604, "outcome": "passed"}}, {"nodeid": "tests/connectivity/test_spark_hdfs.py::TestSparkHDFSIntegration::test_2_4_data_persistence", "lineno": 194, "outcome": "passed", "keywords": ["test_2_4_data_persistence", "TestSparkHDFSIntegration", "test_spark_hdfs.py", "connectivity", "tests", "traffic-prediction", ""], "setup": {"duration": 0.00012689999994108803, "outcome": "passed"}, "call": {"duration": 4.065158500000052, "outcome": "passed", "stdout": "\n\ud83d\udd0d Test 2.4: Data persistence in HDFS\n  Checking HDFS cluster health...\n    \u26a0\ufe0f  Health check: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: /jmx?qry=Hadoop:service=NameNode,name=FSNamesystem (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001697B6EEC90>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n    \u2139\ufe0f  Basic connectivity verified (detailed health check optional)\n  \u2705 Data persistence verified (HDFS operational)\n"}, "teardown": {"duration": 0.00015570000005027396, "outcome": "passed"}}]}